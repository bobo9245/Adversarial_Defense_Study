{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 IMPORT\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TensorBoard 사용하기\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 줄수 제한 없애기\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x100 with 0 Axes>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b02f640>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b02fa60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc4292aef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc4296feb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b073490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b073c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b0d6fb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b10be50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc3b1429b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 6')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc429c7190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class: 1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAABqCAYAAADUSEwwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdqElEQVR4nO29d3Sk13nf/53ee59BGXRgscD2XS53yV1KFElzSUq0DhWZdiQrCqUjKjopPnZiW0c6dmJJSZzEJz6JGUpkrLhIoh2zOZQoNoncpu0FWCz6YAaY3nt/f3/s714O2i4WizaD+zlnj0Rg8GLmwfvee5/2fXgcx3FgMBgMBoPBYDAYjDWGv9lvgMFgMBgMBoPBYDQmzNlgMBgMBoPBYDAY6wJzNhgMBoPBYDAYDMa6wJwNBoPBYDAYDAaDsS4wZ4PBYDAYDAaDwWCsC8zZYDAYDAaDwWAwGOsCczYYDAaDwWAwGAzGusCcDQaDwWAwGAwGg7EuMGeDwWAwGAwGg8FgrAurcjauXbuGL33pS2hra4NUKoVSqcTevXvxn/7Tf0I0GqWvO378OI4fP75W73VDeffdd8Hj8cDj8RAOh9fsuo1oO5fLRW218N+Pf/zjNfs9jWg7ACiVSvijP/ojOJ1OSCQS9Pb24s///M/X7PqNarda2PO6epjt7o5vfvObeOKJJ+BwOMDj8fDbv/3ba/47GtF2Fy9exNe//nUMDAxApVLBYrHg4Ycfxvvvv7+mv6cRbQes/z4BNK7tAGBoaAjPPPMMTCYTJBIJnE4nnn/++TW5dqPabS3XOuHd/sD3v/99PP/88+jp6cHv/u7vYseOHSiVSrhw4QJeeOEFnDlzBq+++uqq39BWIJ1O47nnnoPdbofX612z6za67b7xjW/g2Wefnfe1rq6uNbl2I9vu+eefx1/91V/h3//7f48DBw7g7bffxr/8l/8SqVQKf/AHf3BP125kuxHY87p6mO3unv/23/4bBgcH8dRTT+Hll19e8+s3qu1+9KMf4dy5c/hn/+yfYdeuXchkMnjhhRfwyU9+Ej/84Q/xhS984Z5/R6PaDljffQJobNt98MEHOHHiBB544AG88MILMBqNcLvduHz58j1fu5HttqZrHXcXnD59mhMIBNxjjz3G5fP5Rd8vFArc66+/Tv/72LFj3LFjx+7mV2wJvv71r3N79uzhvvnNb3IAuFAodM/XbGTbTU9PcwC4//yf//O6XL+RbTc0NMTxeDzuO9/5zryvP/fcc5xMJuMikciqr93IdquFPa+rh9nu7qlUKvT/KxQK7otf/OKaXbuRbRcIBBZ9rVwuc4ODg1xHR8c9X7+Rbbee+wTHNbbtMpkMZ7PZuBMnTnDVanVNr93IduO4tV3r7qqM6jvf+Q54PB5efPFFSCSSRd8Xi8V46qmnbnuNP/qjP8KhQ4eg1+uhVquxd+9evPTSS+A4bt7r3n//fRw/fhwGgwEymQwtLS347Gc/i2w2S1/zF3/xF9i1axeUSiVUKhV6e3vv2cP/6KOP8OKLL+IHP/gBBALBPV2rlu1gu/WikW332muvgeM4fOlLX5r39S996UvI5XL42c9+tqrrAo1tNwJ7XlcPs93q4PPXr9WxkW1nNpsXfU0gEGDfvn3weDyrumYtjWy79dwngMa23d/93d/B5/Phd3/3d8Hj8VZ1jeVoZLsBa7vWrbiMqlKp4P3338e+ffvQ3Ny86l/ocrnw1a9+FS0tLQCAs2fP4hvf+Abm5ubwrW99i76GpLxefvllaLVazM3N4Wc/+xmKxSLkcjl+/OMf4/nnn8c3vvEN/Omf/in4fD4mJiZw48aNeb/P6XTSa96JXC6HL3/5y/hX/+pfYe/evXjjjTdW/Tlr2Q62A4Dvfe97+IM/+AMIhULs3bsXv/d7v3fHB+1ONLrthoaGYDKZYLVa5319cHCQfn81NLrdAPa8Mttt3lq3HmxH25XLZXz00Ufo7+9f9ecFGt9267VPAI1vuw8//JB+zqNHj+LcuXNQKBR47LHH8F/+y3+B3W5f1edtdLutOStNgfj9fg4A9/nPf37FaZM7pYwqlQpXKpW4P/7jP+YMBgNNcf393/89B4C7cuXKsj/7L/7Fv+C0Wu0d30NHR8eKU7S/8zu/w7W3t3PZbJbjOI779re/vSalBY1uO6/Xyz333HPcK6+8wn300Ufc3/zN33D33XcfB4D7/ve/f8efvx2NbrtPfepTXE9Pz5LfE4vF3Fe+8pU7XmMpGt1uHMee14Uw2y1mPWxXy1qWUW0323Ecx/3hH/4hB4B77bXXVvXzhEa33XrtExzX+LZ79NFHOQCcVqvlfu/3fo97//33uRdeeIEzGAxcZ2cnl8lk7niNpWh0uy1kQ8uo1oL3338fDz/8MDQaDQQCAUQiEb71rW8hEokgGAwCAHbv3g2xWIyvfOUr+OEPf4ipqalF1zl48CDi8Th+4zd+A6+//vqyKioTExOYmJi44/s6d+4c/uzP/gz/63/9L8hksnv7kOvEVrWdzWbDiy++iGeeeQZHjx7Fs88+iw8//BB79uzBv/t3/w7lcvnePvgasFVtB+C2qd21TvveLVvVbux5XQyz3cY8r1uderHdD37wA/zJn/wJfud3fgef/vSn7/rn14OtbLutvE8AW9d21WoVAPBP/sk/wX/8j/8RDz30EL761a/ipZdewsTEBP72b//2Hj71vbNV7bbmrNQrKZfLnFwu5w4dOrRiT2ahF/erX/2KEwgE3Cc/+UnuJz/5CXfq1Cnu/PnzNLoxPT1NX/vhhx9yTzzxBKdQKDgAXHt7O/dnf/Zn867/8ssvc4cPH+YEAgHH4/G4gwcPcj//+c9X/P5q6e/v55555hkuFovRf//23/5bDgA3OTnJJZPJVV2X4xrfdsvxve99jwPA3bhxY9XXaHTbff7zn+dMJtOir6fTaQ4A9/u///urum6j2409r8x2HLf5a91aZja2k+1efvlljs/nc1/5ylfWpGm30W23XvsEx20P2wHg/uEf/mHe13O5HMfj8bivfe1rq7puo9ttIfe61t2VGtWTTz7JCYVCzuPxrOj1Cw37r//1v+akUimXy+XmvW4pwxLK5TJ39uxZ7jd/8zc5ANyPfvSjRa9Jp9PcW2+9xR04cIATi8Wcy+W6m4/FcRzHAbjtv127dt31NWtpZNstx3e/+10OAHfz5s17uk4j2+5P/uRPOACcz+eb9/UzZ85wALi/+Zu/uetrEhrZbux5ZbarZbPWurVWo9oOtiOOxpe+9KU1VQdqZNut5z7BcY1tu+985zu3dTa+/vWv3/U1CY1st4VsaBnV7//+74PjODz33HMoFouLvl8qlfDmm28u+/M8Hg9CoXCe8kkul8Nf/dVfLfszAoEAhw4dwv/4H/8DAHDp0qVFr1EoFPi1X/s1/OEf/iGKxSKGh4fv5mMBuKXDvPDfF7/4RQC3lCB+8IMf3PU1a2lk2y1FqVTCT37yExiNRnR2dt7TtRrZdp/+9KfB4/Hwwx/+cN7X//Iv/xIymQyPPfbYXV+T0Mh2Y88rs10tm7nWrSWNbru//Mu/xD//5/8cv/Vbv4Uf/OAHa1r+08i2W899Amhs2z399NPg8Xj46U9/Ou/rP/3pT8FxHO677767viahke221tzVUL/Dhw/jL/7iL/D8889j3759+NrXvob+/n6USiVcvnwZL774Inbu3Iknn3xyyZ8/ceIE/ut//a949tln8ZWvfAWRSAR/+qd/ukgy7IUXXsD777+PEydOoKWlBfl8ng4UefjhhwEAzz33HGQyGY4cOQKbzQa/34/vfve70Gg0OHDgAL0WOejeqUZtqamOv/jFLwAAR44cgdFoXJGNlqORbfdv/s2/QalUwpEjR2C1WuHxePDnf/7nuHLlCv73//7f9yyr2ci26+/vx5e//GV8+9vfhkAgwIEDB/Dzn/8cL774Iv7Df/gP0Ov1zG5LwJ5XZrvNsB0A/PKXv0QoFAJwS5FmZmYGf//3fw8AOHbsGEwm011a7GMa2XZ/93d/hy9/+cvYvXs3vvrVr+LcuXPzvr9nz54l5UNXSiPbbj33CaCxbdfb24uvf/3r+J//839CpVLh137t1zA2NoZvfvOb2LNnDz73uc8xuy3Dmq51q0mHXLlyhfviF7/ItbS0cGKxmFMoFNyePXu4b33rW1wwGKSvW6rz/uWXX+Z6eno4iUTCtbe3c9/97ne5l156aV7K6MyZM9zTTz/Ntba2chKJhDMYDNyxY8e4N954g17nhz/8IffQQw9xFouFE4vFnN1u5z73uc9x165dm/f7WltbudbW1tV8zDVTaKmlEW330ksvcQcPHuT0ej0nFAo5nU7HPfroo9zbb7+9ajstRSPajuM4rlgsct/+9rfp5+ru7ub++3//76uy0VI0qt0Wwp5XZruNsN2xY8eWLUH74IMPVmOqRTSi7b74xS/etnxvqZKR1dCItuO49d8nOK5xbVcul7nvfe97XGdnJycSiTibzcZ97Wtf42Kx2GrMtIhGtdtarnU8jlswOYTBYDAYDAaDwWAw1oANl75lMBgMBoPBYDAY2wPmbDAYDAaDwWAwGIx1gTkbDAaDwWAwGAwGY11gzgaDwWAwGAwGg8FYF5izwWAwGAwGg8FgMNYF5mwwGAwGg8FgMBiMdYE5GwwGg8FgMBgMBmNdYM4Gg8FgMBgMBoPBWBeYs8FgMBgMBoPBYDDWBeZsMBgMBoPBYDAYjHWBORsMBoPBYDAYDAZjXWDOBoPBYDAYDAaDwVgXmLPBYDAYDAaDwWAw1gXhZr8BBmMrw3EcAKBarSKbzSKfz0MkEkEikYDP50MkEoHPZz47g8FoDDiOQyAQQDweX/Q9Ho8HtVoNpVIJoVAIqVQKHo+38W+SsS2oVqvI5XIol8tIJpPIZDIQiUSQyWQQiUTQaDQQi8Wb/TYZK4A5GwzGEnAch0qlAo7jUC6XUSqVcOHCBYyNjcFsNqO9vR0KhQIOhwNyuXyz3y6DwWCsCfl8Hv/n//wfvPrqqwAwz5kQiUT41Kc+hfvvvx9msxk9PT0QiUSb9VYZDU4ul8Pw8DDC4TDeffddnDlzBjabDYODgzCbzXj88cfhdDo3+20yVgBzNhiMJahWqyiXy6hUKigUCigUCvD7/ZiamkKpVIJOp0OlUoHFYtnst8pgbFuq1Sqq1So4jkO1WgVw63DM4/EgEAhY1vEuIIGVXC6H8fFxnD17ln6POBwikQjt7e3o6emBTCajNmcw1hLyPBcKBYTDYfh8Pty4cQNnz55FS0sLxGIxMpkM0uk0qtUqfeYZWxfmbDAYSxAIBDA5OYloNIoLFy4gEonA4/EgGAzC4XAgFArBbDZDrVZDIBBALBazCB+DsYGUSiVcu3YNs7Oz8Pl8mJychEAggNFohEKhwJEjR7Bz5052ELkDpVIJ+XwewWAQ77zzDubm5nD16lX6/VrbMTsyNgK/34+bN28iGAzivffew+zsLMbGxgAAiUQC165dQyQSwcGDByEUCmE0GmE0Gjf5XTNuB3M2GIwliEajGBoagsvlwo9+9CPMzc2B4zhwHIempiZks1m0tLRg37590Ol0tH+DwWBsDOVyGSMjI7h06RKuXr2KkydPQiQSoaOjAwaDAWazGX19feDz+RAIBJv9drcspVIJmUwGHo8H//f//l+Mjo4iHo8zJ4OxaYTDYVy8eBGzs7N49913MTs7i0qlAgBIJpPIZrNIJpOYnJyESqWCUCiEwWBg9+kWZsOdjUKhgFgshkKhAI/Hg1QqBYfDgZaWFtr4w1Lf6084HMbMzAwKhQKi0ShKpdKi1/B4PIjFYggEAmi1Wmi1WvD5fPD5fAiFQphMpobrVyBlGclkEn6/H6FQCKVSaV65QDabRSAQQLVaxcmTJ+FyudDZ2YnW1lZIJBKoVCp2DzMYGwwpA0okEuDz+YjFYojFYpBKpVCpVOwgsgwkiFKtVpHP55HL5ZbcDxiM9aRWmGBkZAQjIyMIhULIZrO0f5K8rlKpoFKpIJvNIp1Oo1gsbvK73xyILebm5hAKhVAul5HP5yEWi+FwOKBQKKBQKCCTyTb7rW68s5FMJjE0NIRgMIjXX38do6OjePzxx/H5z38earUadrudqQtsAOPj4/jxj3+MYDCIK1euLKk8QpwMiUSCXbt2YXBwkCqQkDKFlpaWjX/z60ixWES5XEYgEMD169cRCoWQy+XmvSYejyOTyWBiYgIjIyOQy+X47Gc/i0ceeQRGoxE9PT3M2WAwNhgej4disQiv14tEIgGXywWXy0XLqlh2Y3mq1SpKpRKSySQSiQSNIjMYG0W5XMbQ0BCuXr2K69ev4+2330Yul0M2m13UG1StVlEsFhGPxxEMBtHa2rpJ73rzII5GPp/HqVOncPr0aSSTSYRCIeh0Ojz55JNobm5GW1sbHA7HZr/djXc2yuUy0uk0NUogEEA4HEYkEgEA1nC7hlQqFdrknMvlUKlUUCqVUKlUMDs7C6/XS/8GSzkbQqEQhUIBUqkUPp8PBoOByr7K5XLMzs6Cz+dDpVJBo9Fs/AdcYziOo4tbIpFAPB5HOp1etPGSxjViy3Q6Db/fT1O9JpMJMpkMcrkcQuH2q1RcuDGstma+XC5TJbB0Og0AUKvVVHZ4Ozp05N4rl8soFAooFouQSqW0d4hF70HvmVKphGKxyA7OK4RkOMrl8m1fl8/nkUwmkU6nkc1mwXEcxGLxtnweayGR90qlQg/DxD63e/3CZ1YkElHneDs80xzHoVAoIJ/PIxwOY25uDsFgEJlMBvl8ftH9SMQfRCIRtdF2vPc4jkM+n0c2m0U4HIbX60UqlUI4HEahUEAgEIBUKt0yZ+oNPwnl83n4fD7Mzc0hFoshHo/j5s2bePvtt9He3g673b4lUj6NQDweh8/no03O4XAYfr8fkUgEoVAIHo8HhUIB2Wx2yZ8nB+lMJoMrV65gcnISPB4PfD4fYrEYH3zwAbRaLc1M1fvBulQqYXh4GNPT0/jVr36FkZER5HI5FAqFJV/PcRwtsTp58iQmJyfR2tqK+++/HyaTCffffz9sNtsGf4rNhTSbkqgLj8eDTCaDRCJZ8TXIoYc4wi6XCz//+c8BAL/+67+OHTt2QKlUQq1Wr9fH2LLkcjnaHDk8PIyJiQkMDg7ic5/73DxHjMFYD6rVKkZHRwEAnZ2dEAqF0Ol0aG9v35bPI4HjOOrYkiDV2NgYTp8+fdv9YylHorW1FZ/4xCeg1WphMBga/jyUzWYxNjaGSCSCDz74AB9++CESiQR1NBY6axKJBBqNBmazGS0tLWhra4NWq214p2whuVwOExMTiEQiuHjxIs6cOYNKpYJisYhQKIR3330XZrMZMpkMXV1dm/12N97ZqFQqyGQy1GstFouIRqNwu92Qy+V3jKowlmdhRDmXyyESicDv92NoaIhKt3q9Xhr9q5WLXApSCxkMBhEMBunXhUIhvF4vZDIZduzYUdcSiLURPdLL4vP5EIvF6Ocn9lm4QZDa0dnZWVpfajKZkM1msWvXrmU3lEaDbAjE2ahWqzSiTLJhK4X0zaTTafr3IDKchw4dQmtr67YttSyVSggGg/B6vRgaGsKVK1cgEomQzWYhl8u3rV2WorbGm7E2cByHWCyGmZkZyGQyhEIhKpqxXSH9LrVZ2FgsBo/HgytXriwqw639OWDx3pvNZrF7924IBIKGqBi4E8VikZ5TPB4PpqenaVnfUgiFQshkMigUCqjVamg0Gkil0g1+15sP6U+LRCIIBoPw+/30e8ViEW63G5lMBolEYhPf5cfUdyiaQRe4cDiMCxcuIJ1O0+mabrcbIyMjiMfjGB0dRTqdRjQaRbFYnKdLvxp4PB4UCgVUKhVKpRLcbjeUSiWMRmPdZDiy2SxSqRRVtUgkEjh58iQmJibg8XhQrVYhEAggk8kgEAgglUohkUhoypfUS5JStXw+j0AggMuXL2N2dhZ6vR6zs7NwOp0NOXiI3EOlUgkTExP0EExmkRSLRQgEAjz66KPYv38/hELhHZ2OYrGImZkZJBIJnDt3DufPn0cgEIDf74dYLMbVq1dRqVQwMDAAvV6/baL4JFOUTCZx9epVjI2NYXx8HKFQCH6/HzMzM8jn82huboZSqdzst7vpVCoVeL1eDA8Po1Qqwel0sp6NNaBarSKRSKBarUKj0SAcDkMoFG7bICHpGUin05iamkIqlYLL5YLf76dyzEsdmhc6wLUOB8dxeO+992C1WvGpT30KKpVq3T/HZpBKpRCNRuH3+/HRRx9hdnYWHo+HlqEtRKVSQS6Xw+l04siRI7BYLNi5cyeampq2dVZtKSqVChX+yWQyWyLoWR+nQsaylEolZLNZzM7O4s0334Tf74der4dcLsfExASuXr2KUqlE67xvt8jdDaQ8RqPRoFgswuPxQK/XQ6PR1I2zkclkEAwGMTc3h3feeQfBYBCXL1+G2+1GqVRCuVyGWCyGQqGAVCqFVquFSqVCKpVCJBJBsVikmQ+yoYRCIaTTaWg0GiiVSrjdbvB4PLS2tm76w77W1A4Bu3HjBoaHh3Hjxg2cOnWKqtpIpVKYTCbs2LEDUqkUYrH4tnYoFouYnp6mkodvvfUW3XzkcjmuX7+OeDwOpVKJPXv2bBtno7aB98qVK7h8+TLi8TgSiQR1NkqlEoxGI3M2cOve9Hq9uHHjBhQKBevbWEMSiQQSiQT0ej2i0ShkMtm2Va+Kx+MYGxtDKBTC6dOnEQqFcOPGDczMzNCM+XKQMtOFa1gmk0G1WoXNZsPu3bvR2dm53h9jU0ilUnC73XC5XDh58iTcbjfC4fCSjisJbhqNRuzYsQNPP/00zGYz7HY7W++WoFQqIRKJ0B7UrcCGnwprZcvIgygWi+nh7F4PDyTaSlK9AGC326FQKCCRSO6qnGMrUy6XUa1W6UFjamoKPp8PkUgE+XweEomEerakXOpO5QTkUL3UAqhSqaBQKGhJjEgkQnNzMzQaDTo6OqDT6aBUKrd89LBarSIajSKXy8HlcmF8fBzBYBAulwuxWAypVAqlUomqbqlUKvT19UGtVkOv10OpVCKVSlGnYmJiAolEgvZ2kKZ8kuXgOA7t7e2Ym5uDVCqFRqNpiIY2juOQzWbhdruRSCQwNjaGyclJ+P1+5HI5mu2p/beSTFq1WkU8Hqf2Xfhz5O8iFArr2nnL5XK0nFGn00GhUNw281MqlZBKpZBKpZDNZqmNgVs2USqV9BrbBT6fD71eD7vdjpmZmUXRYeIIk0wu484sVXrG4/GWtB85TG8323Ich3g8jlwuh6mpKQwNDSEajWJ2dpb2ayy31olEIojF4nlywwul1UlzebFYpCIQpBG6ESDCKoFAADdu3MDs7Cyi0SjS6fQip5XH49FzW0dHB7q6utDX10f34u203i2ENIjX7gUEUomxlURqNsXZqD0AA4BWq0V7ezscDsc9D0YjZUXXr1/H97//fQgEAvz6r/86Ojs7YbFYYDab1+JjbCpENalQKODcuXN48803EQqFcOXKFWQyGfD5fPB4PGpnYHE/x0L4fD4sFgvsdjsEAgEkEgndvPl8Prq7u9Hb2wu5XA6r1UqbtIgSjlarhUAg2DI39nIUi0UMDQ3B4/HgzJkz+OCDD5DP5xGPx1Eul6n0rUqlgs1mQ2trK/7pP/2naGlpgcFggFqtRiwWg8/ng9/vx6uvvoqZmRm43W74/X6Uy2VkMhkUi0VcvHgRUqmUbjBmsxmDg4OQy+VUSaMeIfXJfr8f//iP/4i5uTmcPn0aExMTVCWptkyPfE0kEt0xnUsyG0QeuxY+nw+1Wg2DwQCFQrGun3G9CYVCePPNN5FMJnHo0CHa5GgymZa0TzqdhtvthtvtRiAQQDQapdF6uVwOh8MBq9W6rWqXhUIhurq6oNPpEAqF5jnwHMfRnh9SRsBgrAXlchnj4+OYmZnBmTNn8NZbb9Ehc7dTP+PxeHSvJM4GKYHO5/Pzrk9UhlKpFBKJBORyed2vecDHz2U2m8WlS5fw13/914jH43C73UsemkUiEUwmE9RqNU6cOIEnnngCarUaFosFQqGwbvfQtaBcLiMej9MAcy21s9FkMtmWCMxtSoN4NptFNptFuVwGn8+HRCKBUqmEXC6/54hvsVhEPp9HIpGA1+sFn8+nQ18aJZVOmmfT6TRCoRC8Xi9isRgymcyyzWjArcVOKBTSadcLHQqTyQSbzQahUDiv3IXP56OpqQlNTU1QKBSwWq0Qi8VQKpUQi8WQSCR1ccgh0c5kMolwOEzr3UulEnK53LwDiVAohEKhgEajgcVigc1mo9EUEn3m8/mw2WwoFApIJBKIxWI0UkVqJYkyxNzcHADQemciVVpPkMbtbDZLS9B8Ph/NqCWTSfpaolhGnC3iAC+36NVKRRLZ4doFlGSD5HI5VCpV3WYoSS8LUYpLJBJLZnAWksvlEAwGEQqF5slBkmeaRP/qPWN2t9TeYwshzzWxV7Va3Xb2WQu2Qr33ZkMqMsj9RNa+UCiEUCiEQqFAM9sLKwP4fD6kUikEAgF0Oh2MRiPNgJO1YOHvItcjket7DcJuFTiOQyaTQTweRzQaRSgUospTS5XikUOzXq+n+zBZ67b7PUn24uUyQjKZjJ7RtgIb7mzEYjGcP38es7OzSKfTkEqlMJvN6O7uhsFguCfDVKtVzM3NYWZmBkNDQ5iZmYFUKkUul1tRGVG9kM/n8eGHH2J0dBRXrlzByMgInftwOwQCAaxWK/R6PZxOJ/bt20cnhBOHwmKx0IMieZh5PB5UKhXUajUtYyFTxPl8fl0cmiuVCgqFAlKpFCYnJ3H16lV4PB7a6L3w3jAYDBgYGEBrayu1GTngyuVy2O126HQ6/NZv/Rbi8Tj+3//7f/jFL36BRCKBubk5qspULBZx4cIFuFwutLa2IhgMwmw24/Dhw2hubt4MU6wKUn6WzWZx7tw5fPjhhwiHw7h+/Tpt9KtFLpejo6MDJpMJra2tMBqNt9WMj0QiGB0dhdfrxcWLFzE8PEzLEUQiEeRyOYxGIwYGBnDw4EE0NTXV5cFxcnIS169fx9TUFH7xi1+gUChg//79kEgkt80Kjo6O4vvf/z5tlgduSUCKxWIa9SRCBtuF2oPLwrpk0iCeSqXgdDoRDAZRKpWg0Wga5uC2UWz3Qx0pZwoEAjh16hTC4TAuXrwIt9tNFQhJIGYplEoljh49CovFgo6ODrS0tCCZTNIgTSqVonOEAKBQKMDv96NQKGB0dBQqlQptbW3QaDR1/7coFou4dOkShoeHcenSJQQCgSUdDTJLw2g04oknnkB3dzf27NkDlUq1LWaPrIRsNouRkRFMTEwgFArN+55UKkVvby+ampq2TDXPhjsbuVwOs7OzmJubo1E50vhD6tlXC8dxNKMRDAZpIymp2W0UZ6NUKsHlctHZF5FIZEVZG+I0mM1mdHZ24vDhw5DL5bQe1Gq1wmAw0IE5jQSJnOfzeSqzF4/Hl63nVigUsFgsMJlMVAWDLHAikQgikQhKpRI6nQ6FQgHj4+O4efMmAMDn8wEAjT7Pzs7Sel6Hw4FUKoWdO3fWVcSQRFESiQTGx8epFrrf76dN8rUIhUJYLBZYrVbodLo7asXncjnMzc3B4/Fgbm4OXq+Xfo9kP0kJn9PphEqlqhvb1RKNRjE6Oorp6Wm4XC7qlN6prC4UCuHChQuIxWJUt59kIMViMb0n69Emq4XMNsjlciiVSvOe49pyDVIPTjLojba2rSW1Aaalvr4dIWWgsVgMw8PD8Hq9uH79+jwhkaWCmcRmMpkMbW1taG9vR19fH9rb2xGJRKBSqeDz+RaVR5HBxzwej8qamkymutovloMEAUZGRuDxeJBOp5dtCBcIBJDL5ejr68OuXbtoVoNxi2KxSDNsC4MtpPxsKzXQb5izkc/nUSgU5jVPkXIIo9FI67BX42yQg1A+n8fo6ChOnz4Nr9cLtVoNnU4Hk8kEo9EIuVxOX08aWNPpNAqFQl1MwSbvO5fL0VKT200oJZBmIZlMhoMHD+Lw4cO0H6G2xIU0XNVjxHg5SHOy3+/H+fPnEQwGce3aNdoQXms7gUAAk8kEjUaD/v5+HDx4EEajEQqFYskSILIgisVi7Nixg8q2KpVKJBIJuN3ueaVF6XQaw8PD8Pl8UCqVmJqaQnt7O7q7u7dstCaTyWBubg6pVAoXL17E3Nwcrl27hmAwiEKhsCiaRw50DocDDzzwAFpaWm6rwU+anaenp3HmzBn4fL5FZQUqlQq9vb2wWq2wWCx1WUZFgh3kvshkMujp6YFIJILD4aCfqfYeqFarCAaDiMVimJ2dpRFAUqphMBhonxWRu26kZ/dOVKtVOoclEoks+X3g40n0S2UwGYylKJfL8Hq9SCaT8Hg8mJycRDAYxMWLFxGLxRAOh1EsFmlGg0ikC4VCmM1maDQaWK1WKp4yODhIS4G0Wi1yuRxtvF9Ohr5cLmNmZgZCoRB6vR67d+/eeEOsEUQwJRaLYXR0FDdv3kQwGFw2G+RwOLBz5040Nzejra0NJpOJnt+2O5FIBF6vF5OTk/B4PPD7/YucDVKyZzabt4zdNszZyOVyiMfjVO0HAG2WMpvNMJvNqy6hIvrzqVQK169fxwcffECbScm1a69frVapyoPP50MymaRazVvxwEeoVqu0caxWsedOCAQCKBQKaLVaPPTQQ/iN3/iNJRWntvJnXy2kfMrtduONN97A7Owsbty4gWAwuOjgIRAI0NzcjKamJuzfvx8PPvgg3UCWsw0pOdu9ezc6OjowNjYGkUhEMye1zkYikcDly5chk8mQTqdhtVrx6KOPor29nTouW41kMomhoSEEAgG89tprGBkZobNJlkImk8FoNKKtrQ2PPPIIurq6ls1qcBxHS7DGxsbw/vvvIxQKLXI21Go1du7cCYfDAbvdvuWDAktRq5I3OTkJiUSCnTt3Qq/Xo6WlBVqtdtHPVKtVeDweTExMYHp6miorAbeeVZPJhN7eXjQ3N0Mmk22Z2tyNgjhjpIxg4cGlVgmNOBsMxkool8uYmpqC2+3GhQsX8NFHHyGdTtMS2YWZDDKAj0TinU4n9u7diyeffJL2opKAFY/HQzqdpvvvclUXxWIRExMTiMfj6OzsrGtHOZvNYnJyEoFAANeuXcPVq1dvW3rW0tKCJ554AjabDd3d3csKZ2xHgsEgLc12uVzw+XyLytBEIhGMRiNsNtv2ymxwHIdwOEw9MZLyJvX+fD7/niJyhUIBMzMzCIVCCAQCNGVO6qDJg07qI/P5PGKxGI0Q8ni8utiIyPR1Uh5A+g1I4yNRZ9Dr9VAoFMhkMkgmk5DJZOjp6YHBYKiroXv3AjnckUY00swXjUapWhKBx+PRRlObzYaOjg7qnK4040D6CtRqNUwmE0ql0qJyDVLKxefz6d8mn89vuRI/EoFPJBLweDwYGxtDMBikyj5ksyXPTu09Z7PZ6BBDjUYDiUSyyIkiKizlchnBYBAejwc+nw/pdJpOH+c4jg5R1Ov1aG1tpRLW9QaRKCwWi9RRU6lUUCqV1EbL/VwoFKKbdO0axefzoVKpYLVaqRLcdoTcK7drrs9ms/D5fKhUKjAajXcs6dsukDWnVoVvJfLUZM8m5bdbNSu7GkgjeD6fh8fjwc2bNzE7O0vlzavVKj031J5d1Go1Ojs7odVq0dPTg+bmZthsNppxXEitvZayHSll1ul0sFqtdbnuEYh6ptvtptmi5WZ+6XQ6qNVqNDU1wWazwWQy3XE203ajUqlQaWTisC1lS6IOulWy3RvmbFy8eBGvvPIK7aUol8vUIPdyI5HG1ddeew03btzA+Pg4otEoVTCoPexcvHiRNrZOTk5CKBTi05/+NHp7e+vC2SD9Ln6/H36/H6FQiEY6hUIhjawcP34cfX19uHnzJi5evAir1Yrf/M3fREtLC7q6ujb5U6w/tYfZ0dFRXLp0CePj47h27Rrt06hFLBbDaDRCq9Xi2LFjeOCBB+git9IHlRyM7XY79uzZA6PRiA8++GDea4hjSJxvkpEjX9sKNbnkPV6/fh2/+tWv4HK58O6771L9+FpVN47jaInefffdh97eXvT399NeICL4sPAzlctlRCIRZDIZnD59GmfPnqUDnWrlD7VaLWw2G3bt2oUTJ07U7aZbqVSo6sr09DTGx8fhdDpht9vR3Ny87ITgUqmE8+fP4yc/+QmV1SQIBAK0tbXhyJEjsNvtrA/hNni9Xrz//vu0pG+pLNJ2hBycSVluKpVa0T5IhAmUSiVkMllDqaCRgF4oFMLbb7+N9957j6pCEUeEHOJ4PB6kUimUSiVaW1vxhS98AU6nEzabDQaDgc6juh1L9cgQB44MLh0cHERPT09d2pgchgOBAN566y3aq7aUUysSibB3714MDAxgcHAQ9913HxQKRV2u+etJuVym5ce14w1qIfNJiAraVmDDnI1UKkXrsWuHUS0nW7gSiMxoLpeD3++nTbjkD0AWB6JYEggEMDMzQwfwyGQyFIvFLeX93Q4SwauVyVwo1yqRSGA2m9HS0oJ0Og2PxwOLxYLm5ma0tLRsmZTaekKcDdLURwQDSEZoISKRCCqVivb3WCwWKJXKuzr4k/uHTBpPpVJ0ECKJHJL3Rsr4yKyUUqm0JWaUkIbbYrGISCRCG9t9Ph9SqdS81wG3FjSRSASpVEqHqzkcDjQ3N9PPQrJuxJa1z2QymaTSzWQAZW22UaVSwWg00n86nW7jjbIGkMxGJpOhm0SxWFx26BK5f0kGlsjd1tqd9FgROeZ6WL/WAzLDRiaTQS6X0wxS7bpYKBQQj8ehVquXbEbdrpDysmw2i1gshmQyuSJnQywWQ61WUwU0ok5Yz5BgT7FYRDKZpM9dIBAAgHllx+QgJxaLaXmy2WxGU1MTWlpaYDQaoVarb/v7aiXSSZUHgZTmkqyu2Wy+6/1oq0BmhpBBuMFgcNFMCLKPkP7d5uZmWCyWZTPjjI+d0+UykUR8iVRnbAU27HRDFjay2EskEvT396O3txdOp3NVDxKRj5uenobb7cbs7CwymQyAW1HBWCyGUqmEl156CTqdDlNTU5icnIRarcaOHTuovGlfX19dPMwKhQLt7e00ZUs0vgOBAL25pFIp2trasHv3brS3t+P++++HXC5HV1fXvBkRjUypVKJO5dmzZ/H2229TLe+lMJlM+MxnPoOWlhbayLdaZR+FQoGWlhYoFAo89thj6OzsxMjICG7evEk39nK5jFgshmw2S5sPNRoNmpubN/Xvk8/ncfnyZQQCAXzwwQf4xS9+gVQqRdWPCKSxUa1Wo7u7GzqdDjt37kRPTw+USiV8Pt+8jVmj0UChUMxzAM+ePQufz4cLFy5gZGQEuVwOlUqFTsOWSqW4//778clPfhIOh6Ouo1uk2ZRIZQKgamYqlWpRsCWdTuP69esIBoOYmZlBJpOh2S8ya4T0xpCa3Ho/7K0GoVCItrY2Wsan1WoRDodx7ty5RX0/jPlUq1U6G+f06dN4/fXX6eyh2yEUCrF7927s378fbW1tOHr0KNRqdV32UdWSy+WQzWYxPT2Nn/3sZ/RcsRRCoRB79+6F0+lEe3s7+vv7odVq0dvbS9euO5FIJHDjxg14vd5FfZcymQw2mw0WiwU7duzArl27tnw/6XJMTU3h2rVrGBsbo5PCF+4nRCZdr9fjyJEjeOCBB6DT6e6qsmA7QQRYcrncovMCmaGmUChgt9vR2tq6bOZ8o9kwZ6NWAQr4eCBaV1fXqpt/8vk89ZYjkcg8rf9SqUT7Gj766CNwHEeHkbW0tODgwYM0Emuz2dbsc64nYrEYJpMJQqEQJpMJJpMJuVwOgUCARo9JYxCJLm+3plHglmNL5G0nJycxPDx824idWq3Gvn370N7eTgcXrhaJRELLhwYGBqDVapHNZuFyuVAoFMDn82lkn8jwElUOu92+6t+7FhSLRbjdbkxNTWFkZAQjIyM0clL7fJJyL6lUCrvdDrPZDLvdDqvVCuDWRkogsrUymYwqqSWTSUxNTcHlcmF6epoOPARAF0qlUonOzk7cf//9de8kk94hv9+PVCpFnQZil4WRp3w+D5fLhdnZWUQikUXrJskmKZVKaLXahipjuRv4fD6MRiMNtPB4PHg8HgwNDTFn4w4QaeBoNIqbN2/i3Xffve1AWAKPx0NLSwv279+PpqYmdHV11fWzSSgWi1R57+TJk7THbymEQiFaW1uxZ88eDAwM4OjRo3edlSbVGGSeRi0ikQh6vZ5Kl9bTPKaFhEIhXL9+nQb/lhK0EYlEdP/o6upCX18fLSVjLEYikUCn00Gj0Sxa90kPkUgkglarhdFo3DJ7w7o6G9VqlTZkE2UesqAJhULIZDIoFIpVNwCVSiUkk8kltZpJ6ULtddVqNex2O9rb2zE4OEhTdfWGSCRCR0cHnYzrcrnoAZbH4+HixYt0InhLSwtNexMt/61y860X5XIZ4XAYs7Oz83oiCOSeIAt5b28vHA4HjEbjmmycpOHc6XRCrVZTeWK/34+LFy/SIVAAMDExgbfeegudnZ1wOp2bIlNHAgGZTAYTExNUretO5HI5uolwHIfh4eFFrxEIBNS2xNGIx+M0g7JwGKBEIkFvby99TtVqdUNNi1Wr1XA6nWhpaYHVaoXRaKSRUFIimUqlMDo6iqmpKYRCoXmSrSTQoNPpoNVqIZVKb6uW1sjweDyo1WoIBAJaAlOpVFj/ym0gAbpUKoVTp05hYmICV69eXdSwu3C93EoCFmsNx3GYnJzE5cuXMTk5CZ/PR6siAFCxGZ1Ohz179sBgMODQoUNob2+HzWZb8X7KcRzcbjd8Ph/9XZFIZJGTJxAIaLaoXu9lUuodjUYxOTkJv9+/7NBhqVSK5uZmNDc3Q6fTNZTgwFpSKBToee+jjz6Cx+OhlTwErVZLgwAk471VbLmuzgY59CUSCaqxXNuvoVAooFarV5R2XApSixuPxxdFrhc2zggEAjidTvT19aG7uxsPPvgg9Ho9DAbD6j/gJiEWizE4OAir1YpkMolz587RsrFUKoW3334bQ0NDGBwcxNGjR6HVatHW1gaFQrEtoqClUgmzs7MYHx9HJBJZcqPk8Xjo6OjAww8/jNbWVnR2dkKv16+JbXg8HmQyGXbs2IFqtQqtVovm5mZcvXoVIyMjyGQytHfjypUrGBoawuHDh/H444/DZDLd8++/W4j6Sjwep43hiUTijso06XQaN27cAI/Hw4ULF5a0HWlkttlsyGQyiMVi9MBD1NRqkclkOHDgAAYHB9Hf3w+DwbDkjJN6hMjV7tixAz09PWhtbYXD4aB2Iyoj0WgU58+fx8jICHXkyD0sFovnKbXUDpvcbvD5fBgMBtq3Qmyx2v1kO5DJZDAyMoJAIIBXXnkFp06doip5hIXr5VYQrlhPOI7D1atX8dd//deIRqOYnp5GoVCga5NEIoFGo0FfXx+ef/55tLS0zJsLttI9o1qtYnh4GKdPn8bo6CiuXbtG94JapFIpnT1Wj1kjjuNoybzP58P169eRTCYXZXAIMpkMvb296OzshMlkYhmNJSCKXtlsFkNDQ/iHf/gHOmutFqPRiIMHD6K1tXXJzMdmsu5lVKQ5trb5U6lUQq1WQ6VS0cjlQmoHWBGpTSL1RWZkuN1uuFyuJVORRJqPlBWRfgen0wmHw0E3p3q8sYnsZblchkajgVKppPM3SDYpFApR2VKVSoVUKkXrvGub10gzGp/Pp8oi9QpZ5AqFAqLRKPx+/5JpW6lUCrFYDIPBgKamJlgslnWpDyWpYDLvxWAw0IZKci+TRXnhBOSNhjg/JLq+kvdCGplvdxARCARIJBIQCAQoFApIJpO0d6M2GCCTyaDVamEymWCz2WhT5FZaLO8Fsg6S+06r1c4bwsdxHO1B83g8dGAnuS+IjYVCIYxG46pEDBoR4ojWSn8vhDhxtZOet5vdyEGFiKT4/X5Eo9El+9gWZjIa1VaktJoELROJBK2SqF0Dyb5JRCo0Gs2qhmiS9ZL8TrL2L5x5RcqlLRZL3TrORMErk8nQkuGFewopIyXKoTqdrm4/73pTrVZpVQAJKpNevlqEQiHkcvmWUqEirHsZFTkEkwniKpUK7e3tsFqt6O7uRldX16Jm3Eqlgrm5OYTDYSSTSYTDYeTzeTopcXZ2lqaCibpBOBye/8H+/0ZTo9GIZ555Bt3d3XQCsVKphNVqpWVF9QYpTTGbzejp6UFPTw+NyORyOaq+5HK5cPr0aTrlWiKR4MCBA+jr66P2FovFVFK0q6sLLS0tm/zpVk+pVEImk0E4HMalS5dw+vTpRSlqsVgMm80GvV6PAwcO4NFHH4VcLl/XBmS73Q69Xo98Pg+bzYZKpYJoNLqiGumNoHZehlAoXFN1NjK9PRKJzHNkFmY0HA4HHnroIdjtdhw9ehTt7e0Ns/GQAEA0GoVer0dHRwdsNhuNxJND8NDQEN544w3anFqr3Ef6EjQaDQ4dOoS+vj60trZu8ierD0qlEs2Ak0PedignrWV2dhbDw8OYmZnBG2+8gVAoNK9XaiHE4WhURwO4dSAm2e+JiQn4/X4656eW5uZmHDlyhJZN6fX6Vd8/hUKBzhNaLqBjMBjw0EMPoampCWazeVWfbTMhUt+xWAxzc3NUon9hGZXJZEJXVxc6OjowMDAAp9NZ10Ig6wmR8R8dHcXQ0BDC4TBdy2oRiUTQaDS0vHQrsa7ORq2EI8lOEDUVUtJDahJJpJdkMqLRKJ3JEQgEqJORTqcxPT1NF4Z0Ok1/BgCt95NKpVCr1dDpdGhvb0dvby/0ej1VGpJKpXW7kJLPR2RWSSmY3++nNiT66bWTsomUnkwmmyfVynEcVCoVLBYLSqUSzQrVG+VymQ5Ni0ajiEQi9Hu1EVByXxgMBpjN5nWXnCX9SSSjQpzc2kP+ZtepEtuQ4YYLa0FXC8dxKBQK8zKPS31OuVyOpqYm2t9Rj71Ut4Nkr4jDoFAowOPx5mVqI5EIVawiayZ5dskzSQQILBYL25hXCBEmIJsz2Ye2A6R2PplMwu/3w+v10gNgNptd8lmslVgmLAwGkswkuae3Um34nSBVEplMBpFIBIFAAPF4fNEzR9ZEpVIJi8VCM9Or7aMgQRYyA6rWqalWq/POLkajESaTqS4rDarVKjKZDBKJBM3iLNWvUVsuptFooFKpNl3+fatCxkeQADzJ1C5EJBLRs8ZWO8Ot61+2WCzi5s2bmJqawtzcHC1zIfXhZ86cgc/no69PJpMYGxujKhnpdJrOIyBRa9IUnslk5qU7yYNLJOM6OzvxiU98Anq9Hjt27IBer6cH9HpaGO/E7t27IZPJEAqFcOnSJSQSCUQiEaTTaXi9XkxOTtKbslwuY2RkZF7zb63U6Cc+8QlEo1EYjUaacaonpqam8Nprr8Hr9WJ2dnbe98RiMZ3wvXfvXnR1daGrq2td7wNyf46Pj2NiYgJDQ0MIhUL03gVu1VharVY4nc5NUw4jg/k0Gg127doFiUSCy5cvIxaLrWii8Fqg1Wqxa9cu2Gy2O2rU1zNEx18qldIJ8idPnsTExARcLhe9P9RqNcRiMVKpFHK5HJRKJcxmM20uJ/LKjDuTSqUwOTkJHo9HJ0EvpQLWaJD9NxQK4fLlyzhz5gwNwmSz2VUNsq1Wq1SlrrOzExzHQafTUTGMemBychLvvvsuIpEIbty4gXA4jJmZGdpDVqlUqJMhkUjQ2tqKnTt3rvrwX61Wkcvl6Nwc0sdaK1zC4/GgUCig0WhgNpshk8nqVvq1WCxiaGgIIyMjmJycXDaDY7FYsH//ftjtdupo1M5jqg20bHeq1SrS6TQdhrsQUgLf0tKCAwcOwGg0brmZauvqbJRKJczNzdGp3qQmnESaRkZG5jkbfr8fv/zlL2n5wFKe23LNvgCo3rrT6cS+ffvwzDPPNFyEdCFtbW1oa2tDKBSCRqNBNBrFzMwMbYyempqiry2Xy/B4PPB4PPRrxHZksB0pc2tra6s7ZyMQCOD999+Hz+ebl9UAbi1YZDHv7u7Grl27YLfb120hq22Sc7vdOHfuHNxuN1VnI4d4tVpNhxhtlr2JZDLpaxIIBIuctfVGqVTSRvLNUOTaKMgQUjLgL5/P49KlSzh16hTy+TxyuRyq1So9bBSLReTzeUilUphMJpjNZphMJhiNxs3+KHVDNptFNpuFSqVCOp2mvTONTrlcxszMDJ11cOXKFeRyOaRSqWWVge4Ex3HweDx0QGpzczPMZjMsFkvdOBt+vx/vvPMOfD4fRkdHl5RJJhkGlUpFnXyi6Hi3kOxuNptFOp1GMpmcp0hY2xui0+looKFehg0vhCgmXbt2DT6fb1mnlswLM5vNkMvl8/rXagPIjSIQcq+QZ3epoYgSiQQqlYqWpmm1Wshksk16p0uzrs6GUCiE3W6nJVB8Ph/lcplG3iuVyrzoXDwep5ttrdQjqSVXKBS0JEUkEtE0KBkYyOPxqPSXwWBo+MhVLUQ+Tq/XQ6PRIJ1O06mmJJNUKBTg9/sRi8Vo7SixM9lEzp07h0QiAaPRCK1WC6vVWjeHPzIoTalULvrb1/YJyGSyZYUJ7pXaVLnX66UypsPDw7R0gZQUArcyGwMDA2hvb9/0lLlQKITVagWPx0NfX9+ySl7kayKRCAaDgZYl1h7gyGZRKpVw6dIljI+PA/i4FpzUPBMJ2P7+fvo3qccN9nbw+Xzo9XrYbDaq+KVSqRCJRMDn8+H1eiEUCtHc3Ayr1YpUKoUrV64gHo/Pc0q7urrQ2tpaN88jY2Px+/0IBAK0dDmbzeL69euYnZ3F3NwcMpkMCoXCPWUrSdlfIpGAz+fDjRs3EAgEUKlUoNfrV3QNEvDZ6MNQbbCT/P/lkEgk2LVrF5xOJx3aV1t+vNLfR6Ssr1y5glAohJs3byIYDM6bt6NUKiEWi9HX14fBwUF0dXXV9VpIZKhJRUotJINDSokjkQhtJheLxbRSJZvNIh6PQyAQwGg0QiaTwWw2Q6/XQywWQ6VSbQsHpFZUIBgMwu12IxaLzXPghEIhuru70dHRgR07dtDz8Vazz7o6GxKJBD09PdDpdDSNnc1maWOay+VaNCxsYS0jcTLkcjnsdjvkcjlsNht0Oh1cLhcuX75MI1fArTKqnTt3oqWlZVvV/ymVSuzcuZMedsmmQGYbjIyMIBaL4dSpUxgeHqZN92QxqFQquHr1Km7evImdO3dCLBbDbrfjgQceqJvDDVH6KRQKS0YuScRErVbDZDKti5oPqQdOp9MYHh6G1+vFhx9+iPfee2+R6hSPx0NbWxseeeQRGAyGTbezWCxGZ2cnlWPV6XS3VaXSarXo7++n/T61kU1ih1QqhT/+4z+mzgZpOiWDh/bt24djx46hvb2dbipbbZG8V4RCIRwOB3g8Ht577z289957tFSDz+fD4XBArVZjYGAAJ06cgNvtptr0ZC20WCw4fPhww5eZMVYHx3EYGxvDyZMnkU6nEQ6Hkcvl4Ha7EY/H6dDb1ZROLSSdTiOTydCSI5lMhqtXr664rK+rqwtWq3XDnQ2y/pKeiVrxhYXI5XI8+uijOHbsGMxmM52ncTdrU23f5E9/+lOMjo5ibGwMMzMz1HZisZgGCI8fP46nn36aRqi34oFxJRBngZSJ1kKqT7RaLQQCAWZmZug0e9K7VqlUEAgEMDExAbFYjN27d8NoNOLQoUMYHByEXq+HXC7fFue7SqVCbTk1NYXr168vEjEQCAS477778Pjjj8Nut2/Z+Szr+tciXnuhUKANueQhu105FGkiJ82qcrkcMpkMdrsdCoUCJpOJRu/FYjEtywJuRa31ev228XwJpBQG+DjyTOwnEolgtVohlUrR0tKCbDYLuVxOSzRIWp1IDRPlHJlMtup0+2ZAIunLyciSRmyJREKHoa0V5L4mGaNEIgGv1wu3202VpxbW55KGfa1Wu2Q2ZqMh6Vgej0dlgW/nbBBJX4VCQWcdEIeORD8XlmyQhnjSP0Ui/kThpRGfWSL3rdPpYLFYYLfbqfyvQCCA2WyG2WyGSqWiUT7yLAIfyxkaDAbodLptscmuB+S+zOVyDaV0RtSNgsEglfsmwaR4PE77fmrLku/lOSMHHSJlXSgUwOfzqU3vdG2dTrdkifR6Q6RYU6kUbVwm2QUCWZuIiIher6fzNFYKWQPT6TT8fj/m5uYQCATm9esRG5Lhu6QhXKvVUkn+el0La8ugFu4ftSJARE2UzGMrFotUKCgYDCISiUAqldKBgG63G0qlEtlsFhqNBlKptOF7rwqFAr13EonEvAAxgcz1UqvVd51920jWvYzKZrNBq9Xi4MGDSKfTNCW0nLNBDjy7du1Cc3MzPTCTmuXaiLVGo8GNGzcQjUbpQ9zc3Ix9+/ZBLpdvSe9uI6jV5CezRvr7+1EqldDR0YFEIoHJyUmcPXsWwWAQv/zlLxEIBGhqORaLYWhoCPF4HEePHt3kT7Nycrkc5ubmqFJZLaQnhah3mc3mNV2kkskkVcFyuVwIBoN49dVXMTY2hng8Pu9+J0MZHQ4H9uzZg9bWVkgkkk0/RBInXyqVor+/H21tbbd9vUAgoBsxedbInJPZ2Vm88sortHYX+Pi+JE2Xer0e+/btw5EjRyCRSBq2jl4kEqGtrY2qbR0/fhyZTIbWM5O65YmJCfz85z/H3Nwc5ubmkEql6ECx5uZm7N27lx5+GHdPsViEx+OBRqOhpSr1TjabxenTpzE7O4vz58/j/PnzyOVytO+RRIrX43BP1ls+nw+Px7PiA7JAIFhyvsd6Uq1W4Xa7MT09jWvXrmFycpKWFteiUqlo5sXpdMJqtd71ukxkXq9du4bXX38dwWAQ58+fpypztSiVSnziE59Af38/+vv7YTQaaVCsESFZDDILbGxsjJZdkSA0KR0iXxsZGYFQKMSNGzegUqkwMDCAz372szAajejv74dOp9vsj7XmEDv4/X688sormJ2dxcjIyJKDcPl8PiQSCRX62apO6rpnNmQyGYRCIcxmM1pbW1EoFOap8dRSG3nu6+tDZ2fnvHpwIltLvDuj0Ugj98RRkcvl9HVb1cPbKGrlXkmjPMk0kR4NAIt6BYrFImKxGJRKZV1lNsgk7KW00snQL5FIRGeO3Au1zX3VahXZbBaJRIIOE/T7/ZiZmcHMzAz9GRLVl0gkMJlMVOaV1LBuhfuVPIMajWZV4gq15VMTExMYHx9HLBajCyCxAYn0GwyGTZmavpGQzAZwqzzDarUinU5Dq9WiXC6jvb0dJpOJqqj5/X5kMhkUi0UqE65SqWAwGBrigLxZVCoVmnVcbppxvUDWHdKHNzMzA6/XSw+0iURi0R5bOzdjpUM7b/f62gj9wgNObYPvQicknU5vmMpdLZlMhkbMSZM2geyVMpmMCjGoVKq7yoCRYB3JTpLDdCgUQjAYXHLALOmTa2tro+eZRoeUSwHzVaeWolqt0gb+SCRCG/e9Xi+9VqNCytGmp6fhcrnmzV0ikLlYIpFoy8+NW1dng5T28Pl89PT0QKFQ0FKX22U2SGO5TqebNxmWGJJIpBEp03w+D4FAsCmp2XqCpCfD4TBu3LiBCxcuUN3mWsrlMrLZLJ3+mclk6HyIrQyZ1SAQCBZtJrWbAPlsxPFYKWRhTCaTCAaDyOVy8Pl8SKfTmJychMfjQSqVoofFUCgE4JYzJ5FIoFar0dbWBp1Oh0ceeYQOUayV/Kt3YrEYXC4XJiYmMD09jdnZWSrVR4Qe1Go17auyWq2b/I43FiJwIRAI4HQ6ae343NwcXC4Xbt68SRvDFQoFHnzwQezduxeDg4MNm/nZKMrlMp3fVLs21CORSATj4+MIBoM4deoUJicn4fP5kEwm55XgEWqFQFay1qzk9QqFAkajERKJBFqtFlKplPZP5vN5RKNRCAQCNDU1QavV0p/buXPnhvdrkAj5W2+9Ba/XuyiIZrPZYLfb4XQ68cgjj8BkMsFut6/4+qlUCsPDw4jH43RA4NTUFMbGxmjJVi0ikYgqCBHZ0s3ObG8UtffWan7O7/fjgw8+gN1uh9VqpZPIG8lRI2eNbDYLl8uFycnJRc4qkao3m83o6+uD2WymZdBbkXW/u4mT4HQ64XQ61+yaJHVEmse3QlR4q8NxHKLRKNxuN8bHx3H9+nVav1oLmYWSy+WQzWaRy+Xm9YRsVaRSKSwWCyqVyjzJX+Djfg7Sp0J6fFbjbKRSKdp4ee3aNYTDYVy/fh1jY2M0c1e72YvFYshkMlgsFuzatQtWqxVHjhxBb29vw6XMU6kUjcTMzs7C6/XO00sXi8VQKpXo6upCd3f3tpNwJX9vYgciExkOhzE3N4fp6Wk6A0EqlWLfvn34zGc+A51Ox5yNe6RardI5ROT5r1fI2uP1enHx4kVMT08jn8+vKNJ7J4dj4SFwudeTPkqlUonm5maoVCpEo1FEo1Ekk0mUy2Xw+Xx0d3fPO7hvhvIekYE/efIk7Q0g8Hg8emDr6+vDww8/TOdyrZRsNouhoSHMzs7i7NmzuHnzJs12L5XFIUEHUrYqkUgaah+4E3fraNT+XDAYxNmzZ+FwOHDs2DHavN9IzgZw67Pm83nMzc3B7XYv+r5SqaRl2B0dHdDpdFvW0QA2wNlYD2pLMpgG8/KQw3WhUEA4HEY2m8XFixcxPj6O8fFxZDKZeY3LBDKEqPZfPRx08vk8wuEwIpHIosgVUXWIx+O4evUqqtUq7eGQSCTQ6XQ02iQSiah8ZLlcpsMkSaNlIBCgkYaZmRkkk0mEw2GaHq5WqxCJRHRae3NzM+x2O8xmMwYGBqDT6aDRaBpquCQhlUrB4/HQpr7ae0uj0aC1tRUOh4PaZKsNHtooSF1yLpfDzZs3MTExgcnJSRSLxXkqfq2trVR2s9HulY1GJBLB4XDA6XTW/fylYrGIeDyOeDy+bKXActzpPiLy1Mu9XqFQQCqVwmq1orOzE2q1Gh0dHdBoNAgGgwiFQojH43Tq/Y4dO9Da2kp/nkRgNxKidEn6KWrh8XgwGo3o7e1Fa2srLf2+nZ3I3hCPxxEKhRAIBDA0NASfz4dAIIBsNotisbjo76LT6aBSqaDRaOBwOGgGhTQ8M+5Mbdmyy+WCUqmk91+jQIKj5XJ50T1EKjJUKhXdS9dDWXOtqUtnAwDtRahVsGLMJ5fLIRwOIxqNzmsGv3btGl14a+dPEIrFIhKJBORyOeLxOGKxGMRi8ZavF0+n0xgdHaWKLLUUCgUacfvxj3+Mn/70p3R6t9FoxO7du6m6kkqlQiKRoOoh09PTNGJP5pTMzc3REhiykZFNjOM4SCQSDAwMwG6349ChQzh48CCUSiUsFgvEYjEd2tRoBAIBnD9/HsFgcNGkU4fDgUcffRQOhwOHDh2Cw+HYVtG8WsrlMlKpFOLxON5++228++67SKVSSKfTaGlpwWc+8xm0t7fTqeosqHLvyOVy7N69G/v27av7ptJsNguPxwOv10tnVpEMxHIlyistobrd6wUCAQwGAywWC7q7u/HQQw/BYDBgYGAAer2eihuEw2HYbDYIBAKcOHECPT09866xGQdrohRYqwQF3Pqs3d3dePzxx6FWq+8oHUoyZKlUCkNDQ/joo48QDAbx0UcfIRqN0jlKC/sR+Hw+2tra0NvbC4fDgf3790On06G/vx96vZ6dYVZIoVCgNj558iQ8Hg8ee+wxtLe3N8waSapLlnJYpVIpdDodmpqasHv3bnR3d694xs1m0hCnndomtmKxiEwmM69xvNEhixrpWSH/HY/H4fV6EYlEMDc3R5vjEokE/dla+xDnjTTlExvWyyRTsVgMnU5HezOII0WcKfK/8Xh8nvpFsViE0WiEWq1GsVhc5GzMzc0hnU7TqFUikUA8Hl9Ss752ACXZlC0WC8xmM2QyGVQqVUMesMkAJ2KbZDKJSqVC7yehUEiHRBK53HrIlq0XpVIJ0WgUkUiEzkDg8XjQaDRUItdisWwJSeR6gKx/t5sjwePxaEljvTv6IpEIarUa2WwWCoWCBlfI519YurPQcVhqXyQlfrfLbAgEAmi1WjrJ3mAw0LWTROxJiZrVaqWv3woZTNJES+RXa52OXC6HWCyGcrlMS5pI9mKhrSqVCsLhMM3i+nw+hMNh6oCQwADZC0jZpFgshsVigcPhoJlujUbTEPdjLaTvdj1Lw8jfkMgZk0N5o5z38vk8YrEYUqnUojWNOBskS0YEZrY6jXOH//+QKdh2u50Op2tkSHQ9n8/T8qFisYhyuYyLFy/inXfeQTKZhNvtRi6XQzQaXfZaZIGwWCzo6+uD3W5HZ2cnWlpa6sKOXV1deP755+Hz+fDqq69iamoKiURinnNVqVSQTCaRyWQQj8cxNzcHsViMS5cu0eyNVCpFLpdDKpWizfKkDItsQMsdalQqFVpbW2EymXD8+HFqv0aWNCQyj263G2fOnMHo6CidViwQCOjGun//fjz44IPQ6XRbPku23oRCIbz55puYnZ3FzZs3kclk0NXVhf3796O5uRn79++nQ0wZd4ZszolEYk0G12112tra8IUvfAGBQABCoRA3b96kKnik/GKlik+kB5L0vEkkkts6G4cPH8bevXtht9uxe/duyGQy6kwQaebm5mZ0dHSAx+NtCbU5Mjuos7MTyWSSyk4Dt/bQd955ByMjIzCbzdi3bx/4fD6uXbuGQCCwqNyVBKhKpRLS6TRisRgNcpLvA6CzcdRqNS2LPHLkCPbv3w+pVAqNRkN7NxoJMjuotbUVmUwGbrd71T0ad4I4Het1/c1iYmKCCj+Q+4oEfZ1OJx544AG0trbCbrdDrVbXhbO69d/hXUCad4PBIORy+abI620kJHJPmp7J0BciDTw9PY0rV67Qco07NQ+SngWFQgGr1Qqr1bplolIrQavVUnWGM2fO0D4KkjonC9JydiAKZyKRCMVicV7j/MLFbGFZC9mQiPQyiWCReRKNtqHUUqlUEAqFMD09Da/XSzffcrlM1afMZjO1iUqlqotIzHqSzWYxMTEBt9tNI6oqlQrt7e1oamqCxWKpi9T4VoEMdl1Kh34hjXAwUavVUKvVMBgMcDgcNNJOghl3yvAs/G8+nw+RSASNRgO5XL6ss8Hn89Hc3IzOzk46iK72WSbKewC2lPgDkbUlAwUXOg9EprypqQkSiQR8Ph+nT59eJF2+HEvdUyR4RXqvLBYLent70dvbu7YfbotBhtaq1Woq3rPWZ7FaWf9G7H2Mx+OYmprC7OwsFTMgn1elUsHpdNIh1/UQCAYazNkAbkW4yETVRthUANChTCSDUalUaNOyx+PB9PQ0kskkXC4XnRRbrVbhcrnowe9OGzBp5NuxYweampqwd+9eOgehXpBIJDAYDBCJRHjqqadw4MABTE5OYmpqCqFQCMPDw7cdJkWadmtLr5ZCqVTCYDDQ3yeTydDU1ASr1QqVSgW73Q6VSkVrKRvV0SBp7HQ6jbGxMVy4cAEul2tezTKfz0dXVxcOHTqE3t5eSKXSup6Oe68kk0nE43HMzMxgamoKXq8XZrMZDocD+/btw549exr6nlkvYrEYRkZG4HK5lg0mFAoFTExMQCqVoqura17Tcr2iUChw/Phx9PT04NSpU+DxeMjlckuKZCwHKZm12Wx46qmnqBz1UmUpfD6fOsR3O1l7M+HxeGhtbcWhQ4cwMTEBn883r/SYQCRseTzeonLj261ZpHSIz+fT4Wp9fX148MEHodVq0dnZCY1Gc1dyuvWKRCLB4OAgjEYjqtUqJiYmkM/nkUql7snpIH8DtVqNpqYm6PV6HD16FC0tLXA6nQ2zp3Ach1AohGvXriESiSCfz1MHTiqVorW1FXv27IHBYKirzHdDORvksJjJZJYc7FavkKnMhUKBZijC4TDS6TQuXryI06dPIxaLYXR0dJ6kY+1gpTvB5/OxY8cOnDhxgqbHt/I0yqUggx+1Wi2MRiNKpRIuXbqEK1euYHR0FJOTk3ecXFsribgcCoUCTU1NUKvV6O7uhk6nw6FDhzAwMACxWAyFQkFlXuuh12W1EFWQeDxOnY10Oj1PHUcgEKCrqwsPPvggHVpVLweU9SCVSmF2dpY6G6FQCIcPH0ZXVxf27NlDy1KYs3F3RKNRjIyMwOfzLStrWygUMDk5CQA02lzvyOVyPPjgg7QHYW5ujvaT1a5lpI9vqUOzTCajU9WffvppdHV13fZ31qNYAZ/PR0tLCw4cOAA+n48zZ87Q4ba1gaV0Oo2RkZF5X1/J5yV9GQKBgO5Bu3btwmc+8xm6H23lGQhriVgsxuDgIHp7e+H1evHhhx8inU4jl8utaH9dDtIHo9Vq0dfXB5vNhvvvv58qojWSbYPBIK5evYpcLodCoUCdDVKmvXfv3rpTKKxrZ4Mc6GobwTOZDMLhMOx2O62tJzV9pFlrK1OtVqmyERkcRwbrZbNZhEIhKnuYz+cxOTmJUCiEdDq9ogwGgdiCROdVKhWam5thNBppLWk93ci1kKZkImnodDohEAhw7Ngx2sRHei/utPgJhULI5XJaW0tkch0OB61NVqlUMJlM9HUkwlWv9lsphUIBgUCANkcutZmQ8gWtVgu5XN7wNrkdZM7N+Pg4PB4PtZVer4fD4YBer6fSy9vZTncDyaAVCgUkk8klp1NLpVKo1WoYjUYoFIqGm2lADmF2ux179uxBJpOhAyNrIQGAhfeWVCqlA1GVSmVDBkhIRNxms8FkMkGr1YLjOCQSiSXniiw1GX0pu0ilUprJMBqNNMttMpnQ09NDS4m2UzaX3I9isRgtLS04evQowuEwxsbGkMlkaLn3Qoe4FnJPkj5H4sRpNBrYbDaaOSGVBY1QllutVpFMJpHP56nASm3QTqfT0dL2ejxf1LWzQQ6C5KYkA19GR0eh1WqRy+UgEomQyWRoTfRW7z8olUq01OKdd96B2+1GNBqlB7rp6el5Skok47FUSvh2CIVCaDQaGAwGPPTQQ3A4HDhw4MC8Upd6hajOiMVi9PT0wOl0olgs4oknnkA2m8XVq1fh9/tpL8vtkMlkcDqdUCqVtO5WKBRCKpXSpspatRHy++ttIVgNqVQKV65cgdfrhcvlQjQaXbRx8/n8eQ5fIx5kVkK1WkW1WsXY2Bhef/11hEIhJBIJCIVCdHV14f7776dleNvpYHKvkPLSRCIBv9+PSCSyaB00Go3YsWMH9Ho97Hb7XQ9sqwd4PB4OHjyInTt33nXTbG3PRqMKN/B4PFrqmsvl0NHRgUAggGKxiFQqterrGo1G9PT0QK/XY//+/TAajejr60NLSwskEgmdf1DP++lqIHvhQw89hH379sHj8eAf//Ef6TwSr9eLVCq17IwYjUaD9vZ26sxJJBLs378f/f39MBgMaGtrg0QioQG+RthXSqUSLfFzuVxUhIDjOFohsHPnTrS3t9fl/VTXzoZIJIJKpUIqlaLd+IVCgR4iw+EwisUiLe0gDbxb+TBIZlxEo1H4/X54vV6Ew2HEYjHE43H4fL5VpSLJwyiRSCAUCqFSqWAwGGAwGGC32+FwOKDT6WiUYKvaZ6WQ90+ms1YqFSiVSqraJRAIkEqloFAobnud2ohfc3MzLBbLPInIeowwrAVEkSUSiSAUCiGbzc475JENlhxi6qWJbT0gQQHybJMSSJlMRlVpSOaHORp3BykPIlluMr+m9p9Op4PNZqPDNOupqfJuIIE3xtIQ6VsiWFGtVpFKpeYNGKxWq3QwKznoicViWgK18FBrsVhgtVppo77BYIDNZoPFYtnoj7elIGuYUqmEQqFApVKBw+GAUChEJBIBx3G0B2EpZ8NqtcLhcFCHTSKR0AF2Go0GRqOxLhSY7gaO45DL5WiVCsnakn4gtVoNk8kElUpVl3tEXf+1rFYrHnjgAbjdbgwNDdGJqqRng2Q2yJTVp556CidOnKCqS1vxDzY3N4e33noLgUAAv/rVr2gNMlGYuhtZx1qpVYFAAJFIhD179qC7uxtNTU0YGBiAUqmEzWaDXC6nSiRb2RlbDeTgSx7a/v5+dHR0oFQq3bGJUiAQQKFQ0NkZ5L6pnWK/3SDyyoFAAGfPnoXL5UIgEJj3GhIIUKvVGz4teKtRLBZx/fp1+Hw+XL58GTMzM9BoNHjsscdgMpmwb98+NDc3b9k1aStDZri0tbXhk5/8JLxeL4rFImKxGDo6OmCz2dDX14fjx49DoVDQUrVGjeAz7ozT6cSzzz6LXC6HRCIxT1AgEolgZGSEzlVKpVLo7u7Gzp07IZVKodVq5x1yST+GTCaDxWKhJXuMW5C90mg04pFHHkEul8PDDz98xzJmktEgs0r4fD4toyK9MY0GcTZImTfJaJD7bmBgAMePH4der6/Lz1/XzgaRigRuedBCoZB6hqQekMfjUWnJgYEBlEqlLZ1yS6VSGB0dhc/nw8zMDMLh8IoOxUtRG10mN21raysGBwfR1dWFw4cP00xHox9yiC2IBjhj9ZCoXzqdhsfjgcvlWjSxncfjQSqVUkdtO1OpVOD3+zE1NYW5uTkkEgnakNvc3Ay73Q6NRrPZb7MuIWu5TqdDR0cHHexZKpXQ1NRESw9IQ+VWXvsZG4NWq8Xu3btRqVSoch5hbm6OnhnI/3Z1deG+++6jkvC1WTEiFU/6H9n9tTRyuRwdHR2b/Ta2NKQUnsjGk6yGSCSizmxbW1vdVp7U9SmANDfn83ns2bMHCoUC4+PjmJ2dRS6XQzgcppOLSbnCVi97sVqteOSRRxCLxdDT04NIJIKhoSEMDQ3RQ96dIDdjd3c33WSJDXp6emgjOIkQbGV7MLYepDRKLpfDaDTStG+tChBxbM1mM7Ra7ea92U2kWq3SxuXp6WkMDQ0BAHbt2gWn04mBgQHY7fZta5+1RKvVoru7G2azGWKxGJlMBg6HA0ajETabrW43aMbaIxAIIJPJaB9VbRkPx3E4cOAAcrkcduzYgWw2i+bmZjidTlrSUxs8EQqFEIvFDVcNwNh4+Hw+LfEjg/q0Wi3uv/9+2Gw2OJ1OiESiusxqAHXubJCaZ6FQiIMHD8JmsyGfz8PlctEhT3K5HG1tbbBardDpdHQIzFalqakJdrsdhUIBLpcL8Xgcf/u3f4vJyUnaGH4nOVuhUAiRSISBgQH89m//NvR6PVpaWmiJFHG42OLIWA2kIZ5E+vL5PKLR6Lxme6lUis7OTjQ1NUGn023em91EyPT5RCKB0dFRXLp0CW1tbTh48CDa29tx8OBBGI3GLb0e1QsGg4HeZ/fdd988mdelau0Z2xfSy7MUGo0Gzc3NtBcIwLz7h+2ZjPVCIBDQuWakXM9qteLEiRNobW1FR0dHXZck17WzAXwsc2qz2cDn8zE4ODhvoZDJZOjt7YXBYIDZbN7yygW1snFEO7qzsxOHDh2iKbbbORtEiUkoFNLPrVKpIJVKG0IejrH5LJw4rNVq6b1V24Rps9ngcDi2vALcelEqlRCPxxGLxajUo1wuh8PhgMlkavg5LBsNsSWzKWO1sCAcY7MgszTK5TLa29tx+PBhNDU1wWaz0SHC9UzdOxvArX6NgwcPolwu48EHH0Q2m6XfI/KkpMGXDKrb6guKUCiExWKB0WjEs88+iyeffJI6UXeSNCSbrVKppJrMzNFgrBXEIVYqlejs7IRMJsPY2BiAW869yWRCR0cHjh07ho6Ojm3bI0MGhAUCAUSjUZRKJTgcDjz00ENUf5/BYDAYDJLJMJvNNKMhEomg0+kaQtGxIZwNPp8PlUoFAA1VskGixFKpdNse2BhbF5LZyOVyVEaZqLOQgUtE/Wc7QgZ0VqtVqlKj1Wqh1+upzC2DwWAwGMDH80mkUilMJtMmv5u1hcetdPIPg8Fg1JDL5RAIBJDNZuHxeBCNRqkmvUajQV9fH1QqFZ1Wv93I5XIIhULI5/Pwer1Ip9NoaWlBR0fHvMZSBoPBYDAaGeZsMBgMBoPBYDAYjHWBddIxGAwGg8FgMBiMdYE5GwwGg8FgMBgMBmNdYM4Gg8FgMBgMBoPBWBeYs8FgMBgMBoPBYDDWBeZsMBgMBoPBYDAYjHWBORsMBoPBYDAYDAZjXWDOBoPBYDAYDAaDwVgXmLPBYDAYDAaDwWAw1gXmbDAYDAaDwWAwGIx1gTkbDAaDwWAwGAwGY11gzgaDwWAwGAwGg8FYF5izwWAwGAwGg8FgMNYF5mwwGAwGg8FgMBiMdeH/AxD7Npb65ieKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Device 및 기본 설정 + 데이터셋 받아오기(MNIST)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data/MNIST',train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='data/MNIST',train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    # print('X_train:', X_train.size(),'type:',X_train.type())\n",
    "    # print('y_train:',y_train.size(), 'type:',y_train.type())\n",
    "    pass\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize)) #10개 plot하기 위한 figure 크기 설정\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1) # plot.subplot(rows, columns, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 4층구조 - Conv - Relu - Pooling 4층\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.gelu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.gelu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        x = F.gelu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.gelu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (bn_fc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn_fc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model,optimizer,criterion 설정\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack Function\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "#FGSM(TSNE 시각화용)\n",
    "def fgsm_attack_2(model, image, label, epsilon, criterion):\n",
    "    # FGSM attack implementation\n",
    "    image.requires_grad = True\n",
    "    output = model(image)\n",
    "    loss = criterion(output, label)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = image.grad.data\n",
    "    perturbed_image = image + epsilon * data_grad.sign()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# PGD Attack Function\n",
    "def pgd_attack(model, image, label, epsilon, alpha, attack_iters, criterion):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True).to(DEVICE)\n",
    "    original_image = image.clone().detach()\n",
    "    \n",
    "    for _ in range(attack_iters):\n",
    "        output = model(perturbed_image)\n",
    "        loss = criterion(output, label)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = perturbed_image.grad.data\n",
    "        \n",
    "        perturbed_image = perturbed_image + alpha * data_grad.sign()\n",
    "        perturbation = torch.clamp(perturbed_image - original_image, min=-epsilon, max=epsilon)\n",
    "        perturbed_image = torch.clamp(original_image + perturbation, 0, 1).detach_().requires_grad_(True)\n",
    "    \n",
    "    return perturbed_image\n",
    "\n",
    "# Adversarial Training with FGSM\n",
    "def train_with_fgsm(model, device, train_loader, optimizer, criterion, epochs, epsilon):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "            \n",
    "            output = model(perturbed_data)\n",
    "            loss_adv = criterion(output, target)\n",
    "            loss_adv.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() + loss_adv.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} with FGSM Attack, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Adversarial Training with PGD\n",
    "def train_with_pgd(model, device, train_loader, optimizer, criterion, epochs, epsilon, alpha, attack_iters):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters, criterion)\n",
    "            output = model(perturbed_data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} with PGD Attack, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Evaluation Function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_predictions.extend(prediction.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=16):\n",
    "    \"\"\"\n",
    "    Visualizes original and adversarial images side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_images_list: List of tensors containing original images.\n",
    "    - perturbed_images_list: List of tensors containing adversarial images.\n",
    "    - original_labels_list: List of original labels.\n",
    "    - perturbed_preds_list: List of adversarial predictions.\n",
    "    - num_images: Number of image pairs to visualize.\n",
    "    \"\"\"\n",
    "    # Concatenate all batches into a single tensor and ensure they are detached\n",
    "    original_images = torch.cat(original_images_list, dim=0)[:num_images].detach()\n",
    "    perturbed_images = torch.cat(perturbed_images_list, dim=0)[:num_images].detach()\n",
    "    original_labels = np.concatenate(original_labels_list, axis=0)[:num_images]\n",
    "    perturbed_preds = np.concatenate(perturbed_preds_list, axis=0)[:num_images]\n",
    "    \n",
    "    # Calculate the number of rows needed\n",
    "    num_cols = 8  # Number of images per row\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate rows needed to display all images\n",
    "\n",
    "    fig, axes = plt.subplots(2 * num_rows, num_cols, figsize=(2 * num_cols, 4 * num_rows))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        row = (i // num_cols) * 2  # Determine the row index (original and adversarial)\n",
    "        col = i % num_cols  # Determine the column index\n",
    "        \n",
    "        # Original Image\n",
    "        axes[row, col].imshow(original_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[row, col].set_title(f\"Original: {original_labels[i]}\")\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        # Adversarial Image\n",
    "        axes[row + 1, col].imshow(perturbed_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[row + 1, col].set_title(f\"Adversarial: {perturbed_preds[i]}\")\n",
    "        axes[row + 1, col].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_images, num_cols * num_rows):\n",
    "        row = (i // num_cols) * 2\n",
    "        col = i % num_cols\n",
    "        axes[row, col].axis('off')\n",
    "        axes[row + 1, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(model, test_loader, epsilon, alpha, iterations, visualize=True, num_images=1000, mode='FGSM'):\n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if i * len(images) >= num_images:\n",
    "            break\n",
    "\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Apply attack based on mode\n",
    "        if mode == 'FGSM':\n",
    "            perturbed_images = fgsm_attack_2(model, images, labels, epsilon, criterion)\n",
    "        elif mode == 'PGD':\n",
    "            perturbed_images = pgd_attack(model, images, labels, epsilon, alpha, iterations, criterion)\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'FGSM' or 'PGD'\")\n",
    "\n",
    "        # Collect original and perturbed images for TSNE\n",
    "        with torch.no_grad():\n",
    "            images_list.append(images)\n",
    "            labels_list.append(labels)\n",
    "            images_list.append(perturbed_images)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Stack all images and labels\n",
    "    images_list = torch.cat(images_list, dim=0)\n",
    "    labels_list = torch.cat(labels_list, dim=0).cpu().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "    # Pass images through the model to get the features\n",
    "    with torch.no_grad():\n",
    "        features = model(images_list).view(images_list.size(0), -1)\n",
    "\n",
    "    # Apply TSNE to reduce to 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_features = tsne.fit_transform(features.cpu().numpy())\n",
    "\n",
    "    # Plot the TSNE visualization\n",
    "    if visualize:\n",
    "        # Colors and markers for each class (0-9)\n",
    "        colors = cm.get_cmap('tab10', 10)  # Using a colormap with 10 colors\n",
    "        markers = ['o', 's', 'v', '^', '<', '>', 'P', '*', 'X', 'D']  # Different markers for diversity\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        for class_idx in range(10):\n",
    "            # Plot original images\n",
    "            class_mask = (labels_list[:len(images_list)//2] == class_idx)\n",
    "            plt.scatter(tsne_features[:len(images_list)//2, 0][class_mask], \n",
    "                        tsne_features[:len(images_list)//2, 1][class_mask], \n",
    "                        color=colors(class_idx), marker=markers[class_idx], label=f'Original {class_idx}', alpha=0.5)\n",
    "            \n",
    "            # Plot perturbed images\n",
    "            class_mask = (labels_list[len(images_list)//2:] == class_idx)\n",
    "            plt.scatter(tsne_features[len(images_list)//2:, 0][class_mask], \n",
    "                        tsne_features[len(images_list)//2:, 1][class_mask], \n",
    "                        color=colors(class_idx), marker=markers[class_idx], label=f'Perturbed {class_idx}', edgecolor='k', alpha=0.5)\n",
    "        \n",
    "        # Set the legend to appear in the top right and split into 2 columns\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1), ncol=2)\n",
    "        plt.title(f'TSNE Visualization - {mode} Attack')\n",
    "        plt.show()\n",
    "\n",
    "    return tsne_features, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with FGSM Attack\n",
    "def evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon, visualize=True, num_images=5):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # For Visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        \n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "        \n",
    "        # FGSM Attack\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        \n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        \n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "        \n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            # Detach tensors to prevent them from tracking gradients\n",
    "            original_images_list.append(data.cpu().detach())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().detach())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    clean_loss /= len(test_loader)\n",
    "    adv_loss /= len(test_loader)\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    \n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    \n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    \n",
    "    print(f\"Adversarial Test Loss: {adv_loss:.4f}, Adversarial Accuracy: {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix:\")\n",
    "    print(adv_cm)\n",
    "    print(len(original_images_list))\n",
    "    return original_images_list,perturbed_images_list,original_labels_list,perturbed_preds_list\n",
    "    # Visualization\n",
    "    if visualize and original_images_list and perturbed_images_list:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n",
    "\n",
    "# Evaluation with PGD Attack\n",
    "def evaluate_with_pgd_attack(model, test_loader, criterion, epsilon, alpha, attack_iters, visualize=True, num_images=5):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # For Visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        \n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "        \n",
    "        # PGD Attack\n",
    "        perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters, criterion)\n",
    "        \n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        \n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "        \n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            original_images_list.append(data.cpu())\n",
    "            perturbed_images_list.append(perturbed_data.cpu())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    clean_loss /= len(test_loader)\n",
    "    adv_loss /= len(test_loader)\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    \n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    \n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    \n",
    "    # Print Results\n",
    "    print(f\"Adversarial Test Loss (PGD): {adv_loss:.4f}, Adversarial Accuracy (PGD): {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro, PGD): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix (PGD):\")\n",
    "    print(adv_cm)\n",
    "    return original_images_list, perturbed_images_list, original_labels_list,perturbed_preds_list\n",
    "    # Visualization\n",
    "    if visualize and original_images_list and perturbed_images_list:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator 모델 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, img_size=28, channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.init_size = img_size // 4  # 초기 이미지 크기 설정\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.model(out)  # self.model 사용\n",
    "        return img\n",
    "\n",
    "# Discriminator 모델 정의 (Critic)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size=28, channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                     nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),   # (1, 28, 28) -> (64, 14, 14)\n",
    "            *discriminator_block(16, 32),                 # (64, 14, 14) -> (128, 7, 7)\n",
    "            *discriminator_block(32, 64),                # (128, 7, 7) -> (256, 4, 4)\n",
    "            *discriminator_block(64, 128),                # (256, 4, 4) -> (512, 2, 2)\n",
    "        )\n",
    "\n",
    "        # 최종 피처 맵의 크기 계산\n",
    "        ds_size = img_size // 2 ** 4  # 최종 피처 맵 크기는 2x2\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(512 * ds_size * ds_size, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)  # Flatten: (batch_size, 2048)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "\n",
    "# Gradient Penalty 계산 함수\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(real_samples.device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(real_samples.device)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WGAN-GP\n",
    "def train_wgan_gp(generator, discriminator, epochs=10, batch_size=64, latent_dim=100, lambda_gp=10, n_critic=5, writer=None):\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (imgs, _) in enumerate(train_loader):\n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            real_imgs = imgs.to(DEVICE)\n",
    "\n",
    "            # 판별자(critic)\n",
    "            optimizer_D.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim).to(DEVICE)\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs)\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # critic step맞춰서\n",
    "            if i % n_critic == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                z = torch.randn(batch_size, latent_dim).to(DEVICE)\n",
    "                gen_imgs = generator(z)\n",
    "                g_loss = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "            # TensorBoard: Log the losses\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_loader) + i\n",
    "                writer.add_scalars('Losses', {'Generator Loss': g_loss.item(), 'Discriminator Loss': d_loss.item()}, global_step)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] | Generator Loss: {g_loss.item():.4f} | Discriminator Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Reconstruction 함수 정의\n",
    "def reconstruct(generator, images, latent_dim, num_iterations=100, lr=0.001, writer=None, log_interval=10):\n",
    "    z = torch.randn(images.size(0), latent_dim, requires_grad=True, device=DEVICE)\n",
    "    optimizer = optim.Adam([z], lr=lr)\n",
    "\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    for i in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        gen_imgs = generator(z)\n",
    "        loss = ((gen_imgs - images) ** 2).mean()  # Mean Squared Error loss\n",
    "        loss.backward(retain_graph=True)  # Retain computation graph for multiple backward passes\n",
    "        optimizer.step()\n",
    "\n",
    "        # TensorBoard: Log the reconstruction loss\n",
    "        if writer:\n",
    "            writer.add_scalar('Reconstruction Loss', loss.item(), i)\n",
    "\n",
    "            # Log reconstructed images at specified intervals\n",
    "            if i % log_interval == 0:\n",
    "                # Normalize the images for better visualization in TensorBoard\n",
    "                grid = torchvision.utils.make_grid(gen_imgs, normalize=True)\n",
    "                writer.add_image(f'Reconstructed Images', grid, i)\n",
    "\n",
    "    reconstructed_images = generator(z.detach())\n",
    "    return reconstructed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defense-GAN Evaluation Function\n",
    "def evaluate_with_defense_gan(model, generator, test_loader, criterion, epsilon, latent_dim, visualize=True, num_images=5, attack_mode='FGSM', writer=None):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss, recon_loss = 0, 0, 0\n",
    "    clean_correct, adv_correct, recon_correct = 0, 0, 0\n",
    "    clean_labels, clean_preds, adv_labels, adv_preds, recon_labels, recon_preds = [], [], [], [], [], []\n",
    "\n",
    "    original_images_list, perturbed_images_list, reconstructed_images_list = [], [], []\n",
    "    original_labels_list, perturbed_preds_list, reconstructed_preds_list = [], [], []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "\n",
    "        # Generate Adversarial Images\n",
    "        if attack_mode == 'FGSM':\n",
    "            data.requires_grad = True\n",
    "            loss = criterion(model(data), target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        elif attack_mode == 'PGD':\n",
    "            perturbed_data = pgd_attack(model, data, target, epsilon, alpha, iterations, criterion)\n",
    "\n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "\n",
    "        # Defense-GAN Reconstruction\n",
    "        reconstructed_data = reconstruct(generator, perturbed_data, latent_dim, writer=writer)\n",
    "\n",
    "        # Reconstructed Evaluation\n",
    "        output = model(reconstructed_data)\n",
    "        recon_loss += criterion(output, target).item()\n",
    "        recon_pred = output.max(1, keepdim=True)[1]\n",
    "        recon_correct += recon_pred.eq(target.view_as(recon_pred)).sum().item()\n",
    "        recon_labels.extend(target.cpu().numpy())\n",
    "        recon_preds.extend(recon_pred.cpu().numpy())\n",
    "\n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            original_images_list.append(data.cpu().detach())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().detach())\n",
    "            reconstructed_images_list.append(reconstructed_data.cpu().detach())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "            reconstructed_preds_list.append(recon_pred.cpu().numpy())\n",
    "\n",
    "    # Calculate Metrics\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    recon_accuracy = 100. * recon_correct / len(test_loader.dataset)\n",
    "\n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    recon_f1 = f1_score(recon_labels, recon_preds, average='macro')\n",
    "\n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    recon_cm = confusion_matrix(recon_labels, recon_preds)\n",
    "\n",
    "    print(f\"Clean Accuracy: {clean_accuracy:.2f}% | Adversarial Accuracy: {adv_accuracy:.2f}% | Reconstructed Accuracy: {recon_accuracy:.2f}%\")\n",
    "    print(f\"Clean F1 Score: {clean_f1:.4f} | Adversarial F1 Score: {adv_f1:.4f} | Reconstructed F1 Score: {recon_f1:.4f}\")\n",
    "    print(\"\\nClean Confusion Matrix:\\n\", clean_cm)\n",
    "    print(\"\\nAdversarial Confusion Matrix:\\n\", adv_cm)\n",
    "    print(\"\\nReconstructed Confusion Matrix:\\n\", recon_cm)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar('Accuracy/Clean', clean_accuracy)\n",
    "        writer.add_scalar('Accuracy/Adversarial', adv_accuracy)\n",
    "        writer.add_scalar('Accuracy/Reconstructed', recon_accuracy)\n",
    "        writer.add_scalar('F1 Score/Clean', clean_f1)\n",
    "        writer.add_scalar('F1 Score/Adversarial', adv_f1)\n",
    "        writer.add_scalar('F1 Score/Reconstructed', recon_f1)\n",
    "        writer.add_image('Confusion Matrix/Clean', clean_cm)\n",
    "        writer.add_image('Confusion Matrix/Adversarial', adv_cm)\n",
    "        writer.add_image('Confusion Matrix/Reconstructed', recon_cm)\n",
    "\n",
    "    if visualize:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n",
    "        visualize_comparison(original_images_list, reconstructed_images_list, original_labels_list, reconstructed_preds_list, num_images=num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (bn_fc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn_fc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1893\n",
      "Epoch 2/10, Loss: 0.0782\n",
      "Epoch 3/10, Loss: 0.0596\n",
      "Epoch 4/10, Loss: 0.0500\n",
      "Epoch 5/10, Loss: 0.0409\n",
      "Epoch 6/10, Loss: 0.0370\n",
      "Epoch 7/10, Loss: 0.0324\n",
      "Epoch 8/10, Loss: 0.0286\n",
      "Epoch 9/10, Loss: 0.0243\n",
      "Epoch 10/10, Loss: 0.0224\n"
     ]
    }
   ],
   "source": [
    "#훈련시의 파라미터\n",
    "\n",
    "#FGSM에서는 attack의 크기 / PGD에서는 어느정도의 제약이라고 생각하면 될듯\n",
    "epsilon=0.2\n",
    "\n",
    "#PGD에서만 사용 : 공격 한번의 크기\n",
    "alpha=0.1\n",
    "iterations=20\n",
    "\n",
    "### Basic Training Execution\n",
    "### Uncomment to perform basic training without adversarial attacks\n",
    "train(model, DEVICE, train_loader, optimizer, criterion, EPOCHS)\n",
    "\n",
    "# Adversarial Training Execution\n",
    "\n",
    "### FGSM Adversarial Training\n",
    "### Uncomment the following line to perform FGSM adversarial training\n",
    "# train_with_fgsm(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon)\n",
    "\n",
    "### PGD Adversarial Training\n",
    "### print(\"Starting PGD Adversarial Training...\")\n",
    "# train_with_pgd(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon, alpha, iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP...\n",
      "Epoch [1/50] | Generator Loss: -0.1426 | Discriminator Loss: 0.1223\n",
      "Epoch [2/50] | Generator Loss: 0.2731 | Discriminator Loss: -0.3520\n",
      "Epoch [3/50] | Generator Loss: 0.6252 | Discriminator Loss: -0.2461\n",
      "Epoch [4/50] | Generator Loss: 1.1428 | Discriminator Loss: -1.0799\n",
      "Epoch [5/50] | Generator Loss: 0.9388 | Discriminator Loss: -0.9141\n",
      "Epoch [6/50] | Generator Loss: 0.7442 | Discriminator Loss: -1.0854\n",
      "Epoch [7/50] | Generator Loss: 2.0095 | Discriminator Loss: -0.8816\n",
      "Epoch [8/50] | Generator Loss: 1.7075 | Discriminator Loss: -0.3833\n",
      "Epoch [9/50] | Generator Loss: 1.1868 | Discriminator Loss: -1.3729\n",
      "Epoch [10/50] | Generator Loss: 1.9152 | Discriminator Loss: -0.7465\n",
      "Epoch [11/50] | Generator Loss: 1.4540 | Discriminator Loss: -0.4558\n",
      "Epoch [12/50] | Generator Loss: 1.1954 | Discriminator Loss: 0.8582\n",
      "Epoch [13/50] | Generator Loss: 1.3388 | Discriminator Loss: -0.1598\n",
      "Epoch [14/50] | Generator Loss: 1.0444 | Discriminator Loss: -0.3146\n",
      "Epoch [15/50] | Generator Loss: 0.4498 | Discriminator Loss: -0.3865\n",
      "Epoch [16/50] | Generator Loss: -0.0065 | Discriminator Loss: 0.2183\n",
      "Epoch [17/50] | Generator Loss: 0.1220 | Discriminator Loss: 0.4989\n",
      "Epoch [18/50] | Generator Loss: -0.0076 | Discriminator Loss: -1.1327\n",
      "Epoch [19/50] | Generator Loss: -0.4786 | Discriminator Loss: 0.0958\n",
      "Epoch [20/50] | Generator Loss: -0.5745 | Discriminator Loss: -0.2019\n",
      "Epoch [21/50] | Generator Loss: -1.2985 | Discriminator Loss: -0.5641\n",
      "Epoch [22/50] | Generator Loss: -1.8651 | Discriminator Loss: -1.3366\n",
      "Epoch [23/50] | Generator Loss: -0.8376 | Discriminator Loss: -0.1261\n",
      "Epoch [24/50] | Generator Loss: -2.0316 | Discriminator Loss: -0.7918\n",
      "Epoch [25/50] | Generator Loss: -1.5807 | Discriminator Loss: -0.5817\n",
      "Epoch [26/50] | Generator Loss: -1.8823 | Discriminator Loss: -0.3541\n",
      "Epoch [27/50] | Generator Loss: -2.1509 | Discriminator Loss: -0.5462\n",
      "Epoch [28/50] | Generator Loss: -2.2472 | Discriminator Loss: -0.8319\n",
      "Epoch [29/50] | Generator Loss: -2.6856 | Discriminator Loss: -0.7076\n",
      "Epoch [30/50] | Generator Loss: -2.6250 | Discriminator Loss: 0.3995\n",
      "Epoch [31/50] | Generator Loss: -3.2983 | Discriminator Loss: -0.6463\n",
      "Epoch [32/50] | Generator Loss: -2.3091 | Discriminator Loss: -0.6736\n",
      "Epoch [33/50] | Generator Loss: -2.7975 | Discriminator Loss: -0.1692\n",
      "Epoch [34/50] | Generator Loss: -2.8598 | Discriminator Loss: -0.1834\n",
      "Epoch [35/50] | Generator Loss: -2.6967 | Discriminator Loss: 0.0915\n",
      "Epoch [36/50] | Generator Loss: -3.5031 | Discriminator Loss: -0.7606\n",
      "Epoch [37/50] | Generator Loss: -3.0389 | Discriminator Loss: -0.5398\n",
      "Epoch [38/50] | Generator Loss: -2.9161 | Discriminator Loss: -0.7281\n",
      "Epoch [39/50] | Generator Loss: -3.5215 | Discriminator Loss: -0.4781\n",
      "Epoch [40/50] | Generator Loss: -3.1544 | Discriminator Loss: -0.7154\n",
      "Epoch [41/50] | Generator Loss: -3.8601 | Discriminator Loss: -0.3803\n",
      "Epoch [42/50] | Generator Loss: -3.8154 | Discriminator Loss: -0.5727\n",
      "Epoch [43/50] | Generator Loss: -4.1190 | Discriminator Loss: -0.8823\n",
      "Epoch [44/50] | Generator Loss: -4.5273 | Discriminator Loss: -0.7244\n",
      "Epoch [45/50] | Generator Loss: -3.9981 | Discriminator Loss: 0.1803\n",
      "Epoch [46/50] | Generator Loss: -4.4366 | Discriminator Loss: -0.2676\n",
      "Epoch [47/50] | Generator Loss: -3.8347 | Discriminator Loss: -0.4471\n",
      "Epoch [48/50] | Generator Loss: -3.5344 | Discriminator Loss: -0.5288\n",
      "Epoch [49/50] | Generator Loss: -4.1235 | Discriminator Loss: -0.2940\n",
      "Epoch [50/50] | Generator Loss: -3.7923 | Discriminator Loss: -0.0209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=6272, bias=True)\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "epochs = 50  # GAN 학습을 위한 에폭 수 설정\n",
    "batch_size = 64\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "generator = Generator(latent_dim=latent_dim, img_size=img_size, channels=channels).to(DEVICE)\n",
    "discriminator = Discriminator(img_size=img_size, channels=channels).to(DEVICE)\n",
    "\n",
    "# 1. WGAN-GP 학습\n",
    "print(\"Training WGAN-GP...\")\n",
    "train_wgan_gp(generator, discriminator, epochs=epochs, batch_size=batch_size, latent_dim=latent_dim, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with Defense-GAN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Defense-GAN 평가\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating with Defense-GAN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mevaluate_with_defense_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFGSM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mevaluate_with_defense_gan\u001b[1;34m(model, generator, test_loader, criterion, epsilon, latent_dim, visualize, num_images, attack_mode, writer)\u001b[0m\n\u001b[0;32m     39\u001b[0m adv_preds\u001b[38;5;241m.\u001b[39mextend(adv_pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Defense-GAN Reconstruction\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m reconstructed_data \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Reconstructed Evaluation\u001b[39;00m\n\u001b[0;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m model(reconstructed_data)\n",
      "Cell \u001b[1;32mIn[25], line 62\u001b[0m, in \u001b[0;36mreconstruct\u001b[1;34m(generator, images, latent_dim, num_iterations, lr, writer, log_interval)\u001b[0m\n\u001b[0;32m     60\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m ((gen_imgs \u001b[38;5;241m-\u001b[39m images) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# 2. Defense-GAN 평가\n",
    "print(\"Evaluating with Defense-GAN...\")\n",
    "evaluate_with_defense_gan(model, generator, test_loader, criterion, epsilon=0.15, latent_dim=latent_dim, visualize=True, num_images=5, attack_mode='FGSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation 함수들 모으 \n",
    "\n",
    "# #공격시의 파라미터 설정\n",
    "# epsilon=0.1\n",
    "# alpha=0.1\n",
    "# iterations=30\n",
    "# # Evaluate on clean test data\n",
    "# print(\"\\n--- Evaluating on Clean Test Data ---\")\n",
    "# evaluate(model, test_loader, criterion)\n",
    "\n",
    "# # Evaluate with FGSM Attack\n",
    "# print(\"\\n--- Evaluating with FGSM Attack ---\")\n",
    "# fgsm_original_images,fgsm_perturbed_images,fgsm_origial_labels,fgsm_perturbed_labels=evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon, visualize=True, num_images=5)\n",
    "\n",
    "# # Evaluate with PGD Attack\n",
    "# print(\"\\n--- Evaluating with PGD Attack ---\")\n",
    "# pgd_original_images,pgd_perturbed_images,pgd_origial_labels,pgd_perturbed_labels=evaluate_with_pgd_attack(model, test_loader, criterion, epsilon, alpha, iterations, visualize=True, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualize 함수\n",
    "\n",
    "\n",
    "# visualize_comparison(fgsm_original_images,fgsm_perturbed_images,fgsm_origial_labels,fgsm_perturbed_labels, num_images=5)\n",
    "\n",
    "# visualize_comparison(pgd_original_images,pgd_perturbed_images,pgd_origial_labels,pgd_perturbed_labels, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FGSM 기반 시각화\n",
    "# tsne_features, labels = visualize_tsne(\n",
    "#     model=model, \n",
    "#     test_loader=test_loader, \n",
    "#     epsilon=epsilon, \n",
    "#     alpha=alpha, \n",
    "#     iterations=iterations, \n",
    "#     visualize=True, \n",
    "#     num_images=2000, \n",
    "#     mode='FGSM'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
