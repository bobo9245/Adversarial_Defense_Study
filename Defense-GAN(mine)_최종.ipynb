{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 IMPORT\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TensorBoard 사용하기\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 줄수 제한 없애기\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device 및 기본 설정 + 데이터셋 받아오기(MNIST)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data/MNIST',train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='data/MNIST',train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    # print('X_train:', X_train.size(),'type:',X_train.type())\n",
    "    # print('y_train:',y_train.size(), 'type:',y_train.type())\n",
    "    pass\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize)) #10개 plot하기 위한 figure 크기 설정\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1) # plot.subplot(rows, columns, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 4층구조 - Conv - Relu - Pooling 4층\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.gelu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.gelu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        x = F.gelu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.gelu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (bn_fc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn_fc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model,optimizer,criterion 설정\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack Function\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "#FGSM(TSNE 시각화용)\n",
    "def fgsm_attack_2(model, image, label, epsilon, criterion):\n",
    "    # FGSM attack implementation\n",
    "    image.requires_grad = True\n",
    "    output = model(image)\n",
    "    loss = criterion(output, label)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = image.grad.data\n",
    "    perturbed_image = image + epsilon * data_grad.sign()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# PGD Attack Function\n",
    "def pgd_attack(model, image, label, epsilon, alpha, attack_iters, criterion):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True).to(DEVICE)\n",
    "    original_image = image.clone().detach()\n",
    "    \n",
    "    for _ in range(attack_iters):\n",
    "        output = model(perturbed_image)\n",
    "        loss = criterion(output, label)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = perturbed_image.grad.data\n",
    "        \n",
    "        perturbed_image = perturbed_image + alpha * data_grad.sign()\n",
    "        perturbation = torch.clamp(perturbed_image - original_image, min=-epsilon, max=epsilon)\n",
    "        perturbed_image = torch.clamp(original_image + perturbation, 0, 1).detach_().requires_grad_(True)\n",
    "    \n",
    "    return perturbed_image\n",
    "\n",
    "# Adversarial Training with FGSM\n",
    "def train_with_fgsm(model, device, train_loader, optimizer, criterion, epochs, epsilon):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "            \n",
    "            output = model(perturbed_data)\n",
    "            loss_adv = criterion(output, target)\n",
    "            loss_adv.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() + loss_adv.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} with FGSM Attack, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Adversarial Training with PGD\n",
    "def train_with_pgd(model, device, train_loader, optimizer, criterion, epochs, epsilon, alpha, attack_iters):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters, criterion)\n",
    "            output = model(perturbed_data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} with PGD Attack, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Evaluation Function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_predictions.extend(prediction.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=16):\n",
    "    \"\"\"\n",
    "    Visualizes original and adversarial images side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_images_list: List of tensors containing original images.\n",
    "    - perturbed_images_list: List of tensors containing adversarial images.\n",
    "    - original_labels_list: List of original labels.\n",
    "    - perturbed_preds_list: List of adversarial predictions.\n",
    "    - num_images: Number of image pairs to visualize.\n",
    "    \"\"\"\n",
    "    # Concatenate all batches into a single tensor and ensure they are detached\n",
    "    original_images = torch.cat(original_images_list, dim=0)[:num_images].detach()\n",
    "    perturbed_images = torch.cat(perturbed_images_list, dim=0)[:num_images].detach()\n",
    "    original_labels = np.concatenate(original_labels_list, axis=0)[:num_images]\n",
    "    perturbed_preds = np.concatenate(perturbed_preds_list, axis=0)[:num_images]\n",
    "    \n",
    "    # Calculate the number of rows needed\n",
    "    num_cols = 8  # Number of images per row\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate rows needed to display all images\n",
    "\n",
    "    fig, axes = plt.subplots(2 * num_rows, num_cols, figsize=(2 * num_cols, 4 * num_rows))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        row = (i // num_cols) * 2  # Determine the row index (original and adversarial)\n",
    "        col = i % num_cols  # Determine the column index\n",
    "        \n",
    "        # Original Image\n",
    "        axes[row, col].imshow(original_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[row, col].set_title(f\"Original: {original_labels[i]}\")\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        # Adversarial Image\n",
    "        axes[row + 1, col].imshow(perturbed_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[row + 1, col].set_title(f\"Adversarial: {perturbed_preds[i]}\")\n",
    "        axes[row + 1, col].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_images, num_cols * num_rows):\n",
    "        row = (i // num_cols) * 2\n",
    "        col = i % num_cols\n",
    "        axes[row, col].axis('off')\n",
    "        axes[row + 1, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(model, test_loader, epsilon, alpha, iterations, visualize=True, num_images=1000, mode='FGSM'):\n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if i * len(images) >= num_images:\n",
    "            break\n",
    "\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Apply attack based on mode\n",
    "        if mode == 'FGSM':\n",
    "            perturbed_images = fgsm_attack_2(model, images, labels, epsilon, criterion)\n",
    "        elif mode == 'PGD':\n",
    "            perturbed_images = pgd_attack(model, images, labels, epsilon, alpha, iterations, criterion)\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be either 'FGSM' or 'PGD'\")\n",
    "\n",
    "        # Collect original and perturbed images for TSNE\n",
    "        with torch.no_grad():\n",
    "            images_list.append(images)\n",
    "            labels_list.append(labels)\n",
    "            images_list.append(perturbed_images)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Stack all images and labels\n",
    "    images_list = torch.cat(images_list, dim=0)\n",
    "    labels_list = torch.cat(labels_list, dim=0).cpu().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "    # Pass images through the model to get the features\n",
    "    with torch.no_grad():\n",
    "        features = model(images_list).view(images_list.size(0), -1)\n",
    "\n",
    "    # Apply TSNE to reduce to 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_features = tsne.fit_transform(features.cpu().numpy())\n",
    "\n",
    "    # Plot the TSNE visualization\n",
    "    if visualize:\n",
    "        # Colors and markers for each class (0-9)\n",
    "        colors = cm.get_cmap('tab10', 10)  # Using a colormap with 10 colors\n",
    "        markers = ['o', 's', 'v', '^', '<', '>', 'P', '*', 'X', 'D']  # Different markers for diversity\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        for class_idx in range(10):\n",
    "            # Plot original images\n",
    "            class_mask = (labels_list[:len(images_list)//2] == class_idx)\n",
    "            plt.scatter(tsne_features[:len(images_list)//2, 0][class_mask], \n",
    "                        tsne_features[:len(images_list)//2, 1][class_mask], \n",
    "                        color=colors(class_idx), marker=markers[class_idx], label=f'Original {class_idx}', alpha=0.5)\n",
    "            \n",
    "            # Plot perturbed images\n",
    "            class_mask = (labels_list[len(images_list)//2:] == class_idx)\n",
    "            plt.scatter(tsne_features[len(images_list)//2:, 0][class_mask], \n",
    "                        tsne_features[len(images_list)//2:, 1][class_mask], \n",
    "                        color=colors(class_idx), marker=markers[class_idx], label=f'Perturbed {class_idx}', edgecolor='k', alpha=0.5)\n",
    "        \n",
    "        # Set the legend to appear in the top right and split into 2 columns\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1), ncol=2)\n",
    "        plt.title(f'TSNE Visualization - {mode} Attack')\n",
    "        plt.show()\n",
    "\n",
    "    return tsne_features, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with FGSM Attack\n",
    "def evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon, visualize=True, num_images=5):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # For Visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        \n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "        \n",
    "        # FGSM Attack\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        \n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        \n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "        \n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            # Detach tensors to prevent them from tracking gradients\n",
    "            original_images_list.append(data.cpu().detach())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().detach())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    clean_loss /= len(test_loader)\n",
    "    adv_loss /= len(test_loader)\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    \n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    \n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    \n",
    "    print(f\"Adversarial Test Loss: {adv_loss:.4f}, Adversarial Accuracy: {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix:\")\n",
    "    print(adv_cm)\n",
    "    print(len(original_images_list))\n",
    "    return original_images_list,perturbed_images_list,original_labels_list,perturbed_preds_list\n",
    "    # Visualization\n",
    "    if visualize and original_images_list and perturbed_images_list:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n",
    "\n",
    "# Evaluation with PGD Attack\n",
    "def evaluate_with_pgd_attack(model, test_loader, criterion, epsilon, alpha, attack_iters, visualize=True, num_images=5):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # For Visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        \n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "        \n",
    "        # PGD Attack\n",
    "        perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters, criterion)\n",
    "        \n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        \n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "        \n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            original_images_list.append(data.cpu())\n",
    "            perturbed_images_list.append(perturbed_data.cpu())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    clean_loss /= len(test_loader)\n",
    "    adv_loss /= len(test_loader)\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    \n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    \n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    \n",
    "    # Print Results\n",
    "    print(f\"Adversarial Test Loss (PGD): {adv_loss:.4f}, Adversarial Accuracy (PGD): {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro, PGD): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix (PGD):\")\n",
    "    print(adv_cm)\n",
    "    return original_images_list, perturbed_images_list, original_labels_list,perturbed_preds_list\n",
    "    # Visualization\n",
    "    if visualize and original_images_list and perturbed_images_list:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성 / 판별자 정의\n",
    "\n",
    "\n",
    "# Generator 모델 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, img_size=28, channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.init_size = img_size // 4  # 초기 이미지 크기 설정\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.model(out)  # self.model 사용\n",
    "        return img\n",
    "\n",
    "# Discriminator 모델 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size=28, channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), \n",
    "                     nn.LeakyReLU(0.2, inplace=True), \n",
    "                     nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),   # (1, 28, 28) -> (64, 14, 14)\n",
    "            *discriminator_block(16, 32),                 # (64, 14, 14) -> (128, 7, 7)\n",
    "            *discriminator_block(32, 64),                # (128, 7, 7) -> (256, 4, 4)\n",
    "            *discriminator_block(64, 128),                # (256, 4, 4) -> (512, 2, 2)\n",
    "        )\n",
    "\n",
    "        # 최종 피처 맵의 크기 계산\n",
    "        ds_size = img_size // 2 ** 4  # 최종 피처 맵 크기는 2x2\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(512 * ds_size * ds_size, 1), nn.Sigmoid())  # 수정: 입력 크기 2048로 설정\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)  # Flatten: (batch_size, 2048)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_GAN + Reconstruction\n",
    "def train_gan(generator, discriminator, epochs=10, batch_size=64, latent_dim=100, writer=None):\n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (imgs, _) in enumerate(train_loader):\n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Real images and labels\n",
    "            real_imgs = imgs.to(DEVICE)\n",
    "            valid = torch.ones(batch_size, 1).to(DEVICE)\n",
    "            fake = torch.zeros(batch_size, 1).to(DEVICE)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim).to(DEVICE)\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion(discriminator(real_imgs), valid)\n",
    "            fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # TensorBoard: Log the losses\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_loader) + i\n",
    "                writer.add_scalars('Losses', {'Generator Loss': g_loss.item(), 'Discriminator Loss': d_loss.item()}, global_step)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] | Generator Loss: {g_loss.item():.4f} | Discriminator Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def reconstruct(generator, images, latent_dim, num_iterations=100, lr=0.001, writer=None, log_interval=10):\n",
    "    z = torch.randn(images.size(0), latent_dim, requires_grad=True, device=DEVICE)\n",
    "    optimizer = optim.Adam([z], lr=lr)\n",
    "\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    for i in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        gen_imgs = generator(z)\n",
    "        loss = ((gen_imgs - images) ** 2).mean()  # Mean Squared Error loss\n",
    "        loss.backward(retain_graph=True)  # Retain computation graph for multiple backward passes\n",
    "        optimizer.step()\n",
    "\n",
    "        # TensorBoard: Log the reconstruction loss\n",
    "        if writer:\n",
    "            writer.add_scalar('Reconstruction Loss', loss.item(), i)\n",
    "\n",
    "            # Log reconstructed images at specified intervals\n",
    "            if i % log_interval == 0:\n",
    "                # Normalize the images for better visualization in TensorBoard\n",
    "                grid = torchvision.utils.make_grid(gen_imgs, normalize=True)\n",
    "                writer.add_image(f'Reconstructed Images', grid, i)\n",
    "\n",
    "    reconstructed_images = generator(z.detach())\n",
    "    return reconstructed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defense-GAN evaluate\n",
    "def evaluate_with_defense_gan(model, generator, test_loader, criterion, epsilon, latent_dim, visualize=True, num_images=5, attack_mode='FGSM'):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss, recon_loss = 0, 0, 0\n",
    "    clean_correct, adv_correct, recon_correct = 0, 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    recon_labels, recon_preds = [], []\n",
    "\n",
    "    # For Visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    reconstructed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "    reconstructed_preds_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Clean Evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "\n",
    "        # Generate Adversarial Images\n",
    "        if attack_mode == 'FGSM':\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = data.grad.data\n",
    "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        elif attack_mode == 'PGD':\n",
    "            perturbed_data = pgd_attack(model, data, target, epsilon, alpha, iterations, criterion)\n",
    "\n",
    "        # Adversarial Evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "\n",
    "        # Defense-GAN Reconstruction\n",
    "        reconstructed_data = reconstruct(generator, perturbed_data, latent_dim)\n",
    "        \n",
    "        # Reconstructed Evaluation\n",
    "        output = model(reconstructed_data)\n",
    "        recon_loss += criterion(output, target).item()\n",
    "        recon_pred = output.max(1, keepdim=True)[1]\n",
    "        recon_correct += recon_pred.eq(target.view_as(recon_pred)).sum().item()\n",
    "        recon_labels.extend(target.cpu().numpy())\n",
    "        recon_preds.extend(recon_pred.cpu().numpy())\n",
    "\n",
    "        # Save for Visualization\n",
    "        if visualize and len(original_images_list) < num_images:\n",
    "            original_images_list.append(data.cpu().detach())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().detach())\n",
    "            reconstructed_images_list.append(reconstructed_data.cpu().detach())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "            reconstructed_preds_list.append(recon_pred.cpu().numpy())\n",
    "\n",
    "    # Calculate Metrics\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "    recon_accuracy = 100. * recon_correct / len(test_loader.dataset)\n",
    "\n",
    "    # F1 Scores\n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "    recon_f1 = f1_score(recon_labels, recon_preds, average='macro')\n",
    "\n",
    "    # Confusion Matrices\n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "    recon_cm = confusion_matrix(recon_labels, recon_preds)\n",
    "    \n",
    "    print(f\"Clean Accuracy: {clean_accuracy:.2f}% | Adversarial Accuracy: {adv_accuracy:.2f}% | Reconstructed Accuracy: {recon_accuracy:.2f}%\")\n",
    "    print(f\"Clean F1 Score: {clean_f1:.4f} | Adversarial F1 Score: {adv_f1:.4f} | Reconstructed F1 Score: {recon_f1:.4f}\")\n",
    "    print(\"\\nClean Confusion Matrix:\\n\", clean_cm)\n",
    "    print(\"\\nAdversarial Confusion Matrix:\\n\", adv_cm)\n",
    "    print(\"\\nReconstructed Confusion Matrix:\\n\", recon_cm)\n",
    "\n",
    "    # Visualization\n",
    "    if visualize and original_images_list and perturbed_images_list:\n",
    "        visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=num_images)\n",
    "        visualize_comparison(original_images_list, reconstructed_images_list, original_labels_list, reconstructed_preds_list, num_images=num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (bn_fc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn_fc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1864\n",
      "Epoch 2/10, Loss: 0.0772\n",
      "Epoch 3/10, Loss: 0.0614\n",
      "Epoch 4/10, Loss: 0.0490\n",
      "Epoch 5/10, Loss: 0.0410\n",
      "Epoch 6/10, Loss: 0.0378\n",
      "Epoch 7/10, Loss: 0.0323\n",
      "Epoch 8/10, Loss: 0.0276\n",
      "Epoch 9/10, Loss: 0.0244\n",
      "Epoch 10/10, Loss: 0.0228\n"
     ]
    }
   ],
   "source": [
    "#훈련시의 파라미터\n",
    "\n",
    "#FGSM에서는 attack의 크기 / PGD에서는 어느정도의 제약이라고 생각하면 될듯\n",
    "epsilon=0.2\n",
    "\n",
    "#PGD에서만 사용 : 공격 한번의 크기\n",
    "alpha=0.1\n",
    "iterations=20\n",
    "\n",
    "### Basic Training Execution\n",
    "### Uncomment to perform basic training without adversarial attacks\n",
    "train(model, DEVICE, train_loader, optimizer, criterion, EPOCHS)\n",
    "\n",
    "# Adversarial Training Execution\n",
    "\n",
    "### FGSM Adversarial Training\n",
    "### Uncomment the following line to perform FGSM adversarial training\n",
    "# train_with_fgsm(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon)\n",
    "\n",
    "### PGD Adversarial Training\n",
    "### print(\"Starting PGD Adversarial Training...\")\n",
    "# train_with_pgd(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon, alpha, iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAN...\n",
      "Epoch [1/50] | Generator Loss: 0.8671 | Discriminator Loss: 0.6040\n",
      "Epoch [2/50] | Generator Loss: 3.5256 | Discriminator Loss: 0.4171\n",
      "Epoch [3/50] | Generator Loss: 1.4801 | Discriminator Loss: 0.4071\n",
      "Epoch [4/50] | Generator Loss: 0.4059 | Discriminator Loss: 0.4262\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "epochs = 50  # GAN 학습을 위한 에폭 수 설정\n",
    "batch_size = 64\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "generator = Generator(latent_dim=latent_dim, img_size=img_size, channels=channels).to(DEVICE)\n",
    "discriminator = Discriminator(img_size=img_size, channels=channels).to(DEVICE)\n",
    "\n",
    "# 1. GAN 학습\n",
    "print(\"Training GAN...\")\n",
    "train_gan(generator, discriminator, epochs=epochs, batch_size=batch_size, latent_dim=latent_dim,writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Defense-GAN 평가\n",
    "print(\"Evaluating with Defense-GAN...\")\n",
    "evaluate_with_defense_gan(model, generator, test_loader, criterion, epsilon=0.2, latent_dim=latent_dim, visualize=True, num_images=5, attack_mode='FGSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 함수들 모으 \n",
    "\n",
    "#공격시의 파라미터 설정\n",
    "epsilon=0.1\n",
    "alpha=0.1\n",
    "iterations=30\n",
    "# Evaluate on clean test data\n",
    "print(\"\\n--- Evaluating on Clean Test Data ---\")\n",
    "evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Evaluate with FGSM Attack\n",
    "print(\"\\n--- Evaluating with FGSM Attack ---\")\n",
    "fgsm_original_images,fgsm_perturbed_images,fgsm_origial_labels,fgsm_perturbed_labels=evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon, visualize=True, num_images=5)\n",
    "\n",
    "# Evaluate with PGD Attack\n",
    "print(\"\\n--- Evaluating with PGD Attack ---\")\n",
    "pgd_original_images,pgd_perturbed_images,pgd_origial_labels,pgd_perturbed_labels=evaluate_with_pgd_attack(model, test_loader, criterion, epsilon, alpha, iterations, visualize=True, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize 함수\n",
    "\n",
    "\n",
    "visualize_comparison(fgsm_original_images,fgsm_perturbed_images,fgsm_origial_labels,fgsm_perturbed_labels, num_images=5)\n",
    "\n",
    "visualize_comparison(pgd_original_images,pgd_perturbed_images,pgd_origial_labels,pgd_perturbed_labels, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM 기반 시각화\n",
    "tsne_features, labels = visualize_tsne(\n",
    "    model=model, \n",
    "    test_loader=test_loader, \n",
    "    epsilon=epsilon, \n",
    "    alpha=alpha, \n",
    "    iterations=iterations, \n",
    "    visualize=True, \n",
    "    num_images=2000, \n",
    "    mode='FGSM'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
