{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 IMPORT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device 및 기본 설정 + 데이터셋 받아오기(MNIST)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data/MNIST',train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='data/MNIST',train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(),'type:',X_train.type())\n",
    "    print('y_train:',y_train.size(), 'type:',y_train.type())\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize)) #10개 plot하기 위한 figure 크기 설정\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1) # plot.subplot(rows, columns, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 4층구조 - Conv - Relu - Pooling 4층\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model,optimizer,criterion 설정\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack Code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Sign of the gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Perturbation with epsilon * sign of gradient\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # Return the perturbed image\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "def train_with_fgsm(model, device, train_loader, optimizer, criterion, epochs, epsilon):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Ensure data requires gradients for FGSM\n",
    "            data.requires_grad_()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Collect the gradient of the input data\n",
    "            data_grad = data.grad.data\n",
    "            \n",
    "            # Create FGSM perturbed data\n",
    "            perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "            \n",
    "            # Re-classify the perturbed image\n",
    "            output = model(perturbed_data)\n",
    "            \n",
    "            # Re-calculate the loss using the perturbed image\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1} completed with FGSM attack.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD Attack Code\n",
    "def pgd_attack(model, image, label, epsilon, alpha, attack_iters):\n",
    "    # Ensure the image requires gradients for PGD\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True).to(DEVICE)\n",
    "    original_image = image.clone().detach()\n",
    "\n",
    "    for _ in range(attack_iters):\n",
    "        # Forward pass to get the loss\n",
    "        output = model(perturbed_image)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # Zero the gradients for each iteration\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Backward pass to get gradients of perturbed image\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the perturbed image using gradient ascent\n",
    "        with torch.no_grad():\n",
    "            perturbed_image = perturbed_image + alpha * perturbed_image.grad.sign()\n",
    "            perturbation = torch.clamp(perturbed_image - original_image, min=-epsilon, max=epsilon)\n",
    "            perturbed_image = torch.clamp(original_image + perturbation, 0, 1)\n",
    "        \n",
    "        # Re-enable gradient tracking for the next iteration\n",
    "        perturbed_image.requires_grad_()\n",
    "\n",
    "    return perturbed_image\n",
    "\n",
    "\n",
    "def train_with_pgd(model, device, train_loader, optimizer, criterion, epochs, epsilon, alpha, attack_iters):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Ensure data requires gradients for PGD\n",
    "            data.requires_grad_()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate PGD perturbed data\n",
    "            perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters)\n",
    "            \n",
    "            # Forward pass with perturbed data\n",
    "            output = model(perturbed_data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} completed with PGD attack.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Image, Perturbated Image 비교코드\n",
    "def visualize_comparison(original_images, perturbed_images, original_labels, perturbed_labels, num_images=5):\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(original_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Original Image - Label: {original_labels[i]}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Adversarial (perturbed) image\n",
    "        axes[i, 1].imshow(perturbed_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(f\"Perturbed Image - Predicted: {perturbed_labels[i]}\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그냥 Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "            # Store the labels and predictions for F1 score and confusion matrix calculation\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_predictions.extend(prediction.cpu().numpy())\n",
    "\n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate F1 Score (macro-averaged)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FGSM, PGD 공격받은 모델에 대한 Evaluation\n",
    "# Evaluation function with adversarial attack (FGSM) and visualization\n",
    "def evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # Store images for visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        # Clean test data evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "\n",
    "        # Store for F1 score and confusion matrix\n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "\n",
    "        # Generate adversarial examples using FGSM\n",
    "        data.requires_grad_()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Adversarial test data evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        # Store for F1 score and confusion matrix\n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "\n",
    "        # Save images and predictions for visualization (only save first 10 examples)\n",
    "        if batch_idx < 10:\n",
    "            original_images_list.append(data.cpu().clone())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().clone())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "\n",
    "    # Calculate average loss\n",
    "    clean_loss /= len(test_loader.dataset) / BATCH_SIZE\n",
    "    adv_loss /= len(test_loader.dataset) / BATCH_SIZE\n",
    "\n",
    "    # Calculate accuracy\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "\n",
    "    # Confusion matrices\n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Clean Test Loss: {clean_loss:.4f}, Clean Accuracy: {clean_accuracy:.2f}%\")\n",
    "    print(f\"Clean F1 Score (Macro): {clean_f1:.4f}\")\n",
    "    print(\"Clean Confusion Matrix:\")\n",
    "    print(clean_cm)\n",
    "\n",
    "    print(f\"Adversarial Test Loss: {adv_loss:.4f}, Adversarial Accuracy: {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix:\")\n",
    "    print(adv_cm)\n",
    "\n",
    "    # Visualize the comparison of original and perturbed images (5 pairs for simplicity)\n",
    "    visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=10)\n",
    "    \n",
    "# Evaluation function with PGD attack and image comparison\n",
    "def evaluate_with_pgd_attack(model, test_loader, criterion, epsilon, alpha, attack_iters):\n",
    "    model.eval()\n",
    "    clean_loss, adv_loss = 0, 0\n",
    "    clean_correct, adv_correct = 0, 0\n",
    "    clean_labels, clean_preds = [], []\n",
    "    adv_labels, adv_preds = [], []\n",
    "    \n",
    "    # Store images for visualization\n",
    "    original_images_list = []\n",
    "    perturbed_images_list = []\n",
    "    original_labels_list = []\n",
    "    perturbed_preds_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        # Clean test data evaluation\n",
    "        output = model(data)\n",
    "        clean_loss += criterion(output, target).item()\n",
    "        clean_pred = output.max(1, keepdim=True)[1]\n",
    "        clean_correct += clean_pred.eq(target.view_as(clean_pred)).sum().item()\n",
    "\n",
    "        # Store for F1 score and confusion matrix\n",
    "        clean_labels.extend(target.cpu().numpy())\n",
    "        clean_preds.extend(clean_pred.cpu().numpy())\n",
    "\n",
    "        # Generate adversarial examples using PGD\n",
    "        perturbed_data = pgd_attack(model, data, target, epsilon, alpha, attack_iters)\n",
    "\n",
    "        # Adversarial test data evaluation\n",
    "        output = model(perturbed_data)\n",
    "        adv_loss += criterion(output, target).item()\n",
    "        adv_pred = output.max(1, keepdim=True)[1]\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        # Store for F1 score and confusion matrix\n",
    "        adv_labels.extend(target.cpu().numpy())\n",
    "        adv_preds.extend(adv_pred.cpu().numpy())\n",
    "\n",
    "        # Save images and predictions for visualization\n",
    "        if batch_idx < 5:  # Save only the first 5 batches for visualization\n",
    "            original_images_list.append(data.cpu().clone())\n",
    "            perturbed_images_list.append(perturbed_data.cpu().clone())\n",
    "            original_labels_list.append(target.cpu().numpy())\n",
    "            perturbed_preds_list.append(adv_pred.cpu().numpy())\n",
    "\n",
    "    # Calculate average loss\n",
    "    clean_loss /= len(test_loader.dataset) / BATCH_SIZE\n",
    "    adv_loss /= len(test_loader.dataset) / BATCH_SIZE\n",
    "\n",
    "    # Calculate accuracy\n",
    "    clean_accuracy = 100. * clean_correct / len(test_loader.dataset)\n",
    "    adv_accuracy = 100. * adv_correct / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    clean_f1 = f1_score(clean_labels, clean_preds, average='macro')\n",
    "    adv_f1 = f1_score(adv_labels, adv_preds, average='macro')\n",
    "\n",
    "    # Confusion matrices\n",
    "    clean_cm = confusion_matrix(clean_labels, clean_preds)\n",
    "    adv_cm = confusion_matrix(adv_labels, adv_preds)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Clean Test Loss: {clean_loss:.4f}, Clean Accuracy: {clean_accuracy:.2f}%\")\n",
    "    print(f\"Clean F1 Score (Macro): {clean_f1:.4f}\")\n",
    "    print(\"Clean Confusion Matrix:\")\n",
    "    print(clean_cm)\n",
    "\n",
    "    print(f\"Adversarial Test Loss (PGD): {adv_loss:.4f}, Adversarial Accuracy (PGD): {adv_accuracy:.2f}%\")\n",
    "    print(f\"Adversarial F1 Score (Macro, PGD): {adv_f1:.4f}\")\n",
    "    print(\"Adversarial Confusion Matrix (PGD):\")\n",
    "    print(adv_cm)\n",
    "\n",
    "    # Visualize the comparison of original and perturbed images\n",
    "    visualize_comparison(original_images_list, perturbed_images_list, original_labels_list, perturbed_preds_list, num_images=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "# 후처리 및 시각화 함수\n",
    "def post_training_visualization(model, data_loader, epsilon=0.3, device='cuda'):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    images, adv_images, labels, adv_labels = [], [], [], []\n",
    "\n",
    "    # 데이터 로더를 통해 이미지를 가져옴\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # 원본 이미지의 예측\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "\n",
    "        # Gradient 계산\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Adversarial example 생성\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1]  # 최종 예측\n",
    "\n",
    "        # 클래스 변경이 있는 경우에만 저장\n",
    "        for i in range(data.size(0)):\n",
    "            if init_pred[i] != final_pred[i]:\n",
    "                images.append(data[i].detach())\n",
    "                adv_images.append(perturbed_data[i].detach())\n",
    "                labels.append(init_pred[i].item())\n",
    "                adv_labels.append(final_pred[i].item())\n",
    "\n",
    "        # 이미지가 5개 이상 모이면 중단\n",
    "        if len(images) >= 5:\n",
    "            break\n",
    "\n",
    "def visualize_adversarial_examples(model, original_images, adversarial_images, original_labels, adversarial_labels, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(len(original_images)):\n",
    "        # 원본 이미지\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.title(f\"Orig: {original_labels[i]}\")\n",
    "        plt.imshow(original_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Adversarial 이미지\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.title(f\"Adv: {adversarial_labels[i]}\")\n",
    "        plt.imshow(adversarial_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공격 없는 학습 실행\n",
    "train(model, DEVICE, train_loader, optimizer, criterion, EPOCHS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "#저 아래 안해도 됨 PGD나 FGSM은 train 말고 일단 test에서 진행해볼 예정\n",
    "\n",
    "\n",
    "\n",
    "# FGSM 학습 실행\n",
    "# epsilon = 0.3  # Perturbation parameter\n",
    "# train_with_fgsm(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon)\n",
    "\n",
    "# PGD 학습 실행\n",
    "# epsilon = 0.3  # Perturbation limit\n",
    "# alpha = 0.01   # Step size\n",
    "# attack_iters = 40  # Number of iterations for PGD\n",
    "# train_with_pgd(model, DEVICE, train_loader, optimizer, criterion, EPOCHS, epsilon, alpha, attack_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model,test_loader)\n",
    "\n",
    "# Example usage after training the model\n",
    "#evaluate_with_fgsm_attack(model, test_loader, criterion, epsilon=0.3)\n",
    "\n",
    "#pgd attack 먹여보기\n",
    "evaluate_with_pgd_attack(model,test_loader,criterion,epsilon=0.3,alpha=0.1,attack_iters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 실행\n",
    "#post_training_visualization(model, test_loader, 3, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
